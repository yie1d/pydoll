{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pydoll","text":""},{"location":"#welcome-to-pydoll","title":"Welcome to Pydoll","text":"<p>Hey there! Thanks for checking out Pydoll, the next generation of browser automation for Python. If you're tired of wrestling with webdrivers and looking for a smoother, more reliable way to automate browsers, you're in the right place.</p>"},{"location":"#what-is-pydoll","title":"What is Pydoll?","text":"<p>Pydoll is revolutionizing browser automation by eliminating the need for webdrivers completely! Unlike other solutions that rely on external dependencies, Pydoll connects directly to browsers using their DevTools Protocol, providing a seamless and reliable automation experience with native asynchronous performance.</p> <p>Whether you're scraping data, testing web applications, or automating repetitive tasks, Pydoll makes it surprisingly easy with its intuitive API and powerful features.</p>"},{"location":"#installation","title":"Installation","text":"<p>Create and activate a virtual environment first, then install Pydoll:</p> <pre><code>$ pip install pydoll-python\n\n---&gt; 100%\n</code></pre> <p>For the latest development version, you can install directly from GitHub:</p> <pre><code>$ pip install git+https://github.com/autoscrape-labs/pydoll.git\n</code></pre>"},{"location":"#why-choose-pydoll","title":"Why Choose Pydoll?","text":"<ul> <li>Zero Webdrivers: Say goodbye to compatibility nightmares and flaky connections</li> <li>Native Captcha Bypass: Smoothly handles Cloudflare Turnstile and reCAPTCHA v3</li> <li>Async Performance: Lightning-fast operations with Python's asyncio</li> <li>Human-like Interactions: Realistic browsing patterns that avoid detection</li> <li>Powerful Event System: React to browser events in real-time</li> <li>Multi-browser Support: Works with Chrome and Edge (more coming soon!)</li> </ul> <p>Ready to dive in? The following pages will guide you through installation, basic usage, and advanced features to help you get the most out of Pydoll.</p> <p>Let's start automating the web, the right way! \ud83d\ude80</p>"},{"location":"#quick-start-guide-a-simple-example","title":"Quick Start Guide: A simple example","text":"<p>Let's start with a practical example. The following script will open the Pydoll GitHub repository and star it:</p> <pre><code>import asyncio\nimport time\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to('https://github.com/autoscrape-labs/pydoll')\n        star_button = await page.wait_element(\n            By.XPATH, '//form[@action=\"/autoscrape-labs/pydoll/star\"]//button',\n            timeout=5,\n            raise_exc=False\n        )\n        if not star_button:\n            print(\"Ops! The button was not found.\")\n            return\n\n        await star_button.click()\n        await asyncio.sleep(3)\n\nasyncio.run(main())\n</code></pre> <p>This example demonstrates how to navigate to a website, wait for an element to appear, and interact with it. You can adapt this pattern to automate many different web tasks.</p> Or use without context manager... <p>If you prefer not to use the context manager pattern, you can manually manage the browser instance:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def main():\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n    await page.go_to('https://github.com/autoscrape-labs/pydoll')\n    star_button = await page.wait_element(\n        By.XPATH, '//form[@action=\"/autoscrape-labs/pydoll/star\"]//button',\n        timeout=5,\n        raise_exc=False\n    )\n    if not star_button:\n        print(\"Ops! The button was not found.\")\n        return\n\n    await star_button.click()\n    await asyncio.sleep(3)\n    await browser.stop()\n\nasyncio.run(main())\n</code></pre> <p>Note that when not using the context manager, you'll need to explicitly call <code>browser.stop()</code> to release resources.</p>"},{"location":"#extended-example-custom-browser-configuration","title":"Extended Example: Custom Browser Configuration","text":"<p>For more advanced usage scenarios, Pydoll allows you to customize your browser configuration using the <code>Options</code> class. This is useful when you need to:</p> <ul> <li>Run in headless mode (no visible browser window)</li> <li>Specify a custom browser executable path</li> <li>Configure proxies, user agents, or other browser settings</li> <li>Set window dimensions or startup arguments</li> </ul> <p>Here's an example showing how to use custom options for Chrome:</p> <pre><code>import asyncio\nimport os\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.browser.options import Options\nfrom pydoll.constants import By\n\nasync def main():\n    options = Options()\n    options.binary_location = '/usr/bin/google-chrome-stable'\n    options.add_argument('--headless=new')\n    options.add_argument('--start-maximized')\n    options.add_argument('--disable-notifications')\n\n    async with Chrome(options=options) as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to('https://github.com/autoscrape-labs/pydoll')\n        star_button = await page.wait_element(\n            By.XPATH, '//form[@action=\"/autoscrape-labs/pydoll/star\"]//button',\n            timeout=5,\n            raise_exc=False\n        )\n        if not star_button:\n            print(\"Ops! The button was not found.\")\n            return\n\n        await star_button.click()\n        await asyncio.sleep(3)\n\n        screenshot_path = os.path.join(os.getcwd(), 'pydoll_repo.png')\n        await page.get_screenshot(screenshot_path)\n        print(f\"Screenshot saved to: {screenshot_path}\")\n\n        repo_description_element = await page.find_element(\n            By.CLASS_NAME, 'f4.my-3'\n        )\n        repo_description = await repo_description_element.text\n        print(f\"Repository description: {repo_description}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>This extended example demonstrates:</p> <ol> <li>Creating and configuring browser options</li> <li>Setting a custom Chrome binary path</li> <li>Enabling headless mode for invisible operation</li> <li>Setting additional browser flags</li> <li>Taking screenshots (especially useful in headless mode)</li> </ol> About Chromium Options <p>The <code>options.add_argument()</code> method allows you to pass any Chromium command-line argument to customize browser behavior. There are hundreds of available options to control everything from networking to rendering behavior.</p> <p>Common Chrome Options</p> <pre><code># Performance &amp; Behavior Options\noptions.add_argument('--headless=new')         # Run Chrome in headless mode\noptions.add_argument('--disable-gpu')          # Disable GPU hardware acceleration\noptions.add_argument('--no-sandbox')           # Disable sandbox (use with caution)\noptions.add_argument('--disable-dev-shm-usage') # Overcome limited resource issues\n\n# Appearance Options\noptions.add_argument('--start-maximized')      # Start with maximized window\noptions.add_argument('--window-size=1920,1080') # Set specific window size\noptions.add_argument('--hide-scrollbars')      # Hide scrollbars\n\n# Network Options\noptions.add_argument('--proxy-server=socks5://127.0.0.1:9050') # Use proxy\noptions.add_argument('--disable-extensions')   # Disable extensions\noptions.add_argument('--disable-notifications') # Disable notifications\n\n# Privacy &amp; Security\noptions.add_argument('--incognito')            # Run in incognito mode\noptions.add_argument('--disable-infobars')     # Disable infobars\n</code></pre> <p>Complete Reference Guides</p> <p>For a comprehensive list of all available Chrome command-line arguments, refer to these resources:</p> <ul> <li>Chromium Command Line Switches - Complete reference list</li> <li>Chrome Flags - Enter this in your Chrome browser address bar to see experimental features</li> <li>Chromium Source Code Flags - Direct source code reference</li> </ul> <p>Remember that some options may behave differently across Chrome versions, so it's a good practice to test your configuration when upgrading Chrome.</p> <p>With these configurations, you can run Pydoll in various environments, including CI/CD pipelines, servers without displays, or Docker containers.</p> <p>Continue reading the documentation to explore Pydoll's powerful features for handling captchas, working with multiple tabs, interacting with elements, and more.</p>"},{"location":"#minimal-dependencies","title":"Minimal Dependencies","text":"<p>One of Pydoll's advantages is its lightweight footprint. Unlike other browser automation tools that require numerous dependencies, Pydoll is intentionally designed to be minimalist while maintaining powerful capabilities.</p>"},{"location":"#core-dependencies","title":"Core Dependencies","text":"<p>Pydoll relies on just a few carefully selected packages:</p> <pre><code>python = \"^3.10\"\nwebsockets = \"^13.1\"\naiohttp = \"^3.9.5\"\naiofiles = \"^23.2.1\"\nbs4 = \"^0.0.2\"\n</code></pre> <p>That's it! This minimal dependency approach means:</p> <ul> <li>Faster installation - No complex dependency tree to resolve</li> <li>Fewer conflicts - Reduced chance of version conflicts with other packages</li> <li>Smaller footprint - Lower disk space usage</li> <li>Better security - Smaller attack surface and dependency-related vulnerabilities</li> <li>Easier updates - Simpler maintenance and fewer breaking changes</li> </ul> <p>The small number of dependencies also contributes to Pydoll's reliability and performance, as there are fewer external factors that could impact its operation.</p>"},{"location":"#license","title":"License","text":"<p>Pydoll is released under the MIT License, which gives you the freedom to use, modify, and distribute the code with minimal restrictions. This permissive license makes Pydoll suitable for both personal and commercial projects.</p> View Full MIT License Text <pre><code>MIT License\n\nCopyright (c) 2023 Pydoll Contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"features/","title":"Key Features","text":"<p>Pydoll brings groundbreaking capabilities to browser automation, making it significantly more powerful than traditional tools while being easier to use.</p>"},{"location":"features/#core-capabilities","title":"Core Capabilities","text":""},{"location":"features/#zero-webdrivers","title":"Zero WebDrivers","text":"<p>Unlike traditional browser automation tools like Selenium, Pydoll eliminates the need for WebDrivers entirely. By connecting directly to browsers through the Chrome DevTools Protocol, Pydoll:</p> <ul> <li>Eliminates version compatibility issues between browser and driver</li> <li>Reduces setup complexity and maintenance overhead</li> <li>Provides more reliable connections without driver-related issues</li> <li>Allows for automation of all Chromium-based browsers with a unified API</li> </ul> <p>No more \"chromedriver version doesn't match Chrome version\" errors or mysterious webdriver crashes.</p>"},{"location":"features/#async-first-architecture","title":"Async-First Architecture","text":"<p>Built from the ground up with Python's asyncio, Pydoll provides:</p> <ul> <li>True Concurrency: Run multiple operations in parallel without blocking</li> <li>Efficient Resource Usage: Manage many browser instances with minimal overhead</li> <li>Modern Python Patterns: Context managers, async iterators, and other asyncio-friendly interfaces</li> <li>Performance Optimizations: Reduced latency and increased throughput for automation tasks</li> </ul>"},{"location":"features/#human-like-interactions","title":"Human-Like Interactions","text":"<p>Avoid detection by mimicking real user behavior:</p> <ul> <li>Natural Typing: Type text with randomized timing between keystrokes</li> <li>Realistic clicking: Click with realistic timing and movement, including offset</li> </ul>"},{"location":"features/#event-driven-capabilities","title":"Event-Driven Capabilities","text":"<p>Respond to browser events in real-time:</p> <ul> <li>Network Monitoring: Track requests, responses, and failed loads</li> <li>DOM Observation: React to changes in the page structure</li> <li>Page Lifecycle Events: Capture navigation, loading, and rendering events</li> <li>Custom Event Handlers: Register callbacks for specific events of interest</li> </ul>"},{"location":"features/#multi-browser-support","title":"Multi-Browser Support","text":"<p>Pydoll works seamlessly with:</p> <ul> <li>Google Chrome: Primary support with all features available</li> <li>Microsoft Edge: Full support for Edge-specific features</li> <li>Chromium: Support for other Chromium-based browsers</li> </ul>"},{"location":"features/#screenshot-and-pdf-export","title":"Screenshot and PDF Export","text":"<p>Capture visual content from web pages:</p> <ul> <li>Full Page Screenshots: Capture entire page content, even beyond the viewport</li> <li>Element Screenshots: Target specific elements for capture</li> <li>High-Quality PDF Export: Generate PDF documents from web pages</li> <li>Custom Formatting: Coming soon!</li> </ul>"},{"location":"features/#native-cloudflare-captcha-bypass","title":"Native Cloudflare Captcha Bypass","text":"<p>Important Information About Captcha Bypass</p> <p>The effectiveness of Cloudflare Turnstile bypass depends on several factors:</p> <ul> <li>IP Reputation: Cloudflare assigns a \"trust score\" to each IP address. Clean residential IPs typically receive higher scores.</li> <li>Previous History: IPs with a history of suspicious activity may be permanently flagged.</li> </ul> <p>Pydoll can achieve scores comparable to a regular browser session, but cannot overcome IP-based blocks or extremely restrictive configurations. For best results, use residential IPs with good reputation.</p> <p>Remember that captcha bypass techniques operate in a gray area and should be used responsibly.</p> <p>One of Pydoll's most powerful features is its ability to automatically bypass Cloudflare Turnstile captchas that block most automation tools:</p>"},{"location":"features/#context-manager-approach-synchronous","title":"Context Manager Approach (Synchronous)","text":"<pre><code>import asyncio\nfrom pydoll.browser import Chrome\n\nasync def bypass_cloudflare_example():\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n\n    # The context manager will wait for the captcha to be processed\n    # before continuing execution\n    async with page.expect_and_bypass_cloudflare_captcha():\n        await page.go_to('https://site-with-cloudflare.com')\n        print(\"Waiting for captcha to be handled...\")\n\n    # This code runs only after the captcha is successfully bypassed\n    print(\"Captcha bypassed! Continuing with automation...\")\n    await page.find_element_by_id('protected-content').get_text()\n\n    await browser.stop()\n\nasyncio.run(bypass_cloudflare_example())\n</code></pre>"},{"location":"features/#background-processing-approach","title":"Background Processing Approach","text":"<pre><code>import asyncio\nfrom pydoll.browser import Chrome\n\nasync def background_bypass_example():\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n\n    # Enable automatic captcha solving before navigating\n    await page.enable_auto_solve_cloudflare_captcha()\n\n    # Navigate to the protected site - captcha handled automatically in background\n    await page.go_to('https://site-with-cloudflare.com')\n    print(\"Page loaded, captcha will be handled in the background...\")\n\n    # Add a small delay to allow captcha solving to complete\n    await asyncio.sleep(3)\n\n    # Continue with automation\n    await page.find_element_by_id('protected-content').get_text()\n\n    # Disable auto-solving when no longer needed\n    await page.disable_auto_solve_cloudflare_captcha()\n\n    await browser.stop()\n\nasyncio.run(background_bypass_example())\n</code></pre> <p>Access websites that actively block automation tools without using third-party captcha solving services. This native captcha handling makes Pydoll suitable for automating previously inaccessible websites.</p>"},{"location":"features/#concurrent-scraping","title":"Concurrent Scraping","text":"<p>Pydoll's async architecture allows you to scrape multiple pages or websites simultaneously for maximum efficiency:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def scrape_page(url):\n    \"\"\"Process a single page and extract data\"\"\"\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to(url)\n\n        # Extract data\n        title = await page.execute_script('return document.title')\n\n        # Find elements and extract content\n        elements = await page.find_elements(By.CSS_SELECTOR, '.article-content')\n        content = []\n        for element in elements:\n            text = await element.get_element_text()\n            content.append(text)\n\n        return {\n            \"url\": url,\n            \"title\": title,\n            \"content\": content\n        }\n\nasync def main():\n    # List of URLs to scrape in parallel\n    urls = [\n        'https://example.com/page1',\n        'https://example.com/page2',\n        'https://example.com/page3',\n        'https://example.com/page4',\n        'https://example.com/page5',\n    ]\n\n    # Process all URLs concurrently\n    results = await asyncio.gather(*(scrape_page(url) for url in urls))\n\n    # Print results\n    for result in results:\n        print(f\"Scraped {result['url']}: {result['title']}\")\n        print(f\"Found {len(result['content'])} content blocks\")\n\n    return results\n\n# Run the concurrent scraping\nall_data = asyncio.run(main())\n</code></pre> <p>This approach provides dramatic performance improvements over sequential scraping, especially for I/O-bound tasks like web scraping. Instead of waiting for each page to load one after another, Pydoll processes them all simultaneously, reducing total execution time significantly. For example, scraping 10 pages that each take 2 seconds to load would take just over 2 seconds total instead of 20+ seconds with sequential processing.</p>"},{"location":"features/#advanced-keyboard-control","title":"Advanced Keyboard Control","text":"<p>Pydoll provides human-like keyboard interaction with precise control over typing behavior:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\nfrom pydoll.common.keys import Keys\n\nasync def realistic_typing_example():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to('https://example.com/login')\n\n        # Find login form elements\n        username = await page.find_element(By.ID, 'username')\n        password = await page.find_element(By.ID, 'password')\n\n        # Type with realistic timing (interval between keystrokes)\n        await username.type_keys(\"user@example.com\", interval=0.15)\n\n        # Use special key combinations\n        await password.click()\n        await password.key_down(Keys.SHIFT)\n        await password.send_keys(\"PASSWORD\")\n        await password.key_up(Keys.SHIFT)\n\n        # Press Enter to submit\n        await password.send_keys(Keys.ENTER)\n\n        # Wait for navigation\n        await asyncio.sleep(2)\n        print(\"Logged in successfully!\")\n\nasyncio.run(realistic_typing_example())\n</code></pre> <p>This realistic typing helps avoid detection by websites that look for automation patterns. The natural timing and ability to use special key combinations makes Pydoll's interactions virtually indistinguishable from human users.</p>"},{"location":"features/#powerful-event-system","title":"Powerful Event System","text":"<p>Pydoll's event system allows you to react to browser events in real-time:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.network import NetworkEvents\nfrom pydoll.events.page import PageEvents\nfrom functools import partial\n\nasync def event_monitoring_example():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n\n        # Monitor page load events\n        async def on_page_loaded(event):\n            print(f\"\ud83c\udf10 Page loaded: {event['params'].get('url')}\")\n\n        await page.enable_page_events()\n        await page.on(PageEvents.PAGE_LOADED, on_page_loaded)\n\n        # Monitor network requests\n        async def on_request(page, event):\n            url = event['params']['request']['url']\n            print(f\"\ud83d\udd04 Request to: {url}\")\n\n        await page.enable_network_events()\n        await page.on(NetworkEvents.REQUEST_WILL_BE_SENT, \n                      partial(on_request, page))\n\n        # Navigate and see events in action\n        await page.go_to('https://example.com')\n        await asyncio.sleep(5)  # Allow time to see events\n\nasyncio.run(event_monitoring_example())\n</code></pre> <p>The event system makes Pydoll uniquely powerful for monitoring API requests and responses, creating reactive automations, debugging complex web applications, and building comprehensive web monitoring tools.</p>"},{"location":"features/#file-upload-support","title":"File Upload Support","text":"<p>Seamlessly handle file uploads in your automation:</p> <pre><code>import asyncio\nimport os\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def file_upload_example():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to('https://example.com/upload')\n\n        # Method 1: Direct file input\n        file_input = await page.find_element(By.XPATH, '//input[@type=\"file\"]')\n        await file_input.set_input_files('path/to/document.pdf')\n\n        # Method 2: Using file chooser with an upload button\n        sample_file = os.path.join(os.getcwd(), 'sample.jpg')\n        async with page.expect_file_chooser(files=sample_file):\n            upload_button = await page.find_element(By.ID, 'upload-button')\n            await upload_button.click()\n\n        # Submit the form\n        submit = await page.find_element(By.ID, 'submit-button')\n        await submit.click()\n\n        print(\"Files uploaded successfully!\")\n\nasyncio.run(file_upload_example())\n</code></pre> <p>File uploads are notoriously difficult to automate in other frameworks, often requiring workarounds. Pydoll makes it straightforward with both direct file input and file chooser dialog support.</p>"},{"location":"features/#multi-browser-example","title":"Multi-Browser Example","text":"<p>Pydoll works with different browsers through a consistent API:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.browser.edge import Edge\n\nasync def multi_browser_example():\n    # Run the same automation in Chrome\n    async with Chrome() as chrome:\n        await chrome.start()\n        chrome_page = await chrome.get_page()\n        await chrome_page.go_to('https://example.com')\n        chrome_title = await chrome_page.execute_script('return document.title')\n        print(f\"Chrome title: {chrome_title}\")\n\n    # Run the same automation in Edge\n    async with Edge() as edge:\n        await edge.start()\n        edge_page = await edge.get_page()\n        await edge_page.go_to('https://example.com')\n        edge_title = await edge_page.execute_script('return document.title')\n        print(f\"Edge title: {edge_title}\")\n\nasyncio.run(multi_browser_example())\n</code></pre> <p>Cross-browser compatibility without changing your code. Test your automations across different browsers to ensure they work everywhere.</p>"},{"location":"features/#proxy-integration","title":"Proxy Integration","text":"<p>Unlike many automation tools that struggle with proxy implementation, Pydoll offers native proxy support with full authentication capabilities. This makes it ideal for:</p> <ul> <li>Web scraping projects that need to rotate IPs</li> <li>Geo-targeted testing of applications across different regions</li> <li>Privacy-focused automation that requires anonymizing traffic</li> <li>Testing web applications through corporate proxies</li> </ul> <p>Configuring proxies in Pydoll is straightforward:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.browser.options import Options\n\nasync def proxy_example():\n    # Create browser options\n    options = Options()\n\n    # Simple proxy without authentication\n    options.add_argument('--proxy-server=192.168.1.100:8080')\n    # Or proxy with authentication\n    # options.add_argument('--proxy-server=username:password@192.168.1.100:8080')\n\n    # Bypass proxy for specific domains\n    options.add_argument('--proxy-bypass-list=*.internal.company.com,localhost')\n\n    # Start browser with proxy configuration\n    async with Chrome(options=options) as browser:\n        await browser.start()\n        page = await browser.get_page()\n\n        # Test the proxy by visiting an IP echo service\n        await page.go_to('https://api.ipify.org')\n        ip_address = await page.page_source\n        print(f\"Current IP address: {ip_address}\")\n\n        # Continue with your automation\n        await page.go_to('https://example.com')\n        title = await page.execute_script('document.title')\n        print(f\"Page title: {title}\")\n\nasyncio.run(proxy_example())\n</code></pre>"},{"location":"features/#request-interception","title":"Request Interception","text":"<p>Intercept and modify network requests before they're sent:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.fetch import FetchEvents\nfrom pydoll.commands.fetch import FetchCommands\nfrom functools import partial\n\nasync def request_interception_example():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n\n        # Define the request interceptor\n        async def intercept_request(page, event):\n            request_id = event['params']['requestId']\n            url = event['params']['request']['url']\n\n            if '/api/' in url:\n                # Get original headers\n                original_headers = event['params']['request'].get('headers', {})\n\n                # Add custom headers\n                custom_headers = {\n                    **original_headers,\n                    'Authorization': 'Bearer my-token-123',\n                    'X-Custom-Header': 'CustomValue'\n                }\n\n                print(f\"\ud83d\udd04 Modifying request to: {url}\")\n                await page._execute_command(\n                    FetchCommands.continue_request(\n                        request_id=request_id,\n                        headers=custom_headers\n                    )\n                )\n            else:\n                # Continue normally for non-API requests\n                await page._execute_command(\n                    FetchCommands.continue_request(\n                        request_id=request_id\n                    )\n                )\n\n        # Enable interception and register handler\n        await page.enable_fetch_events()\n        await page.on(FetchEvents.REQUEST_PAUSED, \n                      partial(intercept_request, page))\n\n        # Navigate to trigger requests\n        await page.go_to('https://example.com')\n        await asyncio.sleep(5)  # Allow time for requests to process\n\nasyncio.run(request_interception_example())\n</code></pre> <p>This powerful capability allows you to add authentication headers dynamically, modify request payloads before they're sent, mock API responses for testing, and analyze and debug network traffic.</p> <p>Each of these features showcases what makes Pydoll a next-generation browser automation tool, combining the power of direct browser control with an intuitive, async-native API. </p>"},{"location":"deep-dive/","title":"Deep Dive","text":"<p>Welcome to the in-depth technical documentation section of Pydoll. This area is dedicated to developers who want to understand the internal workings of the library, its architectural design, and the technical principles behind its operation.</p>"},{"location":"deep-dive/#what-youll-find-here","title":"What You'll Find Here","text":"<p>Unlike the introduction and features sections that focus on \"how to use\" Pydoll, the Deep Dive section explores \"how it works\" and the \"why\" behind the design and implementation decisions.</p> <p>In this section, you'll find detailed documentation about:</p> <ul> <li>Chrome DevTools Protocol (CDP) - How Pydoll communicates with browsers without relying on webdrivers</li> <li>Internal Architecture - The layered structure that makes Pydoll efficient and extensible</li> <li>Domain Implementations - Technical details of each functional domain (Browser, Page, WebElement)</li> <li>Event System - How the reactive event system works internally</li> <li>Performance Optimizations - Details about how we achieve high asynchronous performance</li> </ul>"},{"location":"deep-dive/#who-this-section-is-for","title":"Who This Section Is For","text":"<p>This documentation is especially useful for:</p> <ul> <li>Developers looking to contribute code to Pydoll</li> <li>Engineers creating advanced integrations or extensions</li> <li>Technical users who need to understand the execution model for debugging</li> <li>Anyone interested in the technical aspects of browser automation</li> </ul> <p>Each topic in this section is self-contained, so you can navigate directly to the areas of greatest interest using the navigation menu.</p> <p>Explore the different domains and technical features using the sidebar links to dive deep into Pydoll's implementation details.</p>"},{"location":"deep-dive/browser-domain/","title":"Browser Domain","text":"<p>The Browser domain is the backbone of Pydoll's zero-webdriver architecture. This component provides a direct interface to browser instances through the Chrome DevTools Protocol (CDP), eliminating the need for traditional webdrivers while delivering superior performance and reliability.</p> <pre><code>graph LR\n    A[Pydoll API] --&gt; B[Browser Domain]\n    B &lt;--&gt; C[Chrome DevTools Protocol]\n    C &lt;--&gt; D[Browser Process]\n\n    subgraph \"Internal Components\"\n        B --&gt; E[Connection Handler]\n        B --&gt; F[Process Manager]\n        B --&gt; G[Options Manager]\n        B --&gt; H[Proxy Manager]\n    end</code></pre>"},{"location":"deep-dive/browser-domain/#technical-architecture","title":"Technical Architecture","text":"<p>At its core, the Browser domain is implemented as an abstract base class (<code>Browser</code>) that establishes the fundamental contract for all browser implementations. Specific browser classes like <code>Chrome</code> and <code>Edge</code> extend this base class to provide browser-specific behavior while sharing the common architecture.</p> <pre><code># Abstract base class (simplified)\nclass Browser(ABC):\n    def __init__(self, options=None, connection_port=None, browser_type=None):\n        # Initialize components\n        # ...\n\n    @abstractmethod\n    def _get_default_binary_location(self) -&gt; str:\n        \"\"\"Must be implemented by subclasses\"\"\"\n        pass\n\n    async def start(self):\n        # Start browser process\n        # Establish CDP connection\n        # Configure initial state\n        # ...\n\n# Implementation for Chrome\nclass Chrome(Browser):\n    def _get_default_binary_location(self) -&gt; str:\n        # Return path to Chrome binary\n        # ...\n</code></pre> <p>The abstraction allows Pydoll to support multiple browsers through a unified interface, with each implementation handling browser-specific details such as executable discovery, command-line arguments, and protocol variations.</p>"},{"location":"deep-dive/browser-domain/#core-usage-patterns","title":"Core Usage Patterns","text":"<p>The Browser domain follows a consistent pattern for initialization, page management, and cleanup:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\n\nasync def simple_browser_example():\n    # Create and start a browser instance\n    browser = Chrome()\n    await browser.start()\n\n    try:\n        # Get a page and navigate to a URL\n        page = await browser.get_page()\n        await page.go_to(\"https://example.com\")\n\n        # Perform operations with the page\n        title = await page.execute_script(\"return document.title\")\n        print(f\"Page title: {title}\")\n\n    finally:\n        # Always ensure the browser is properly closed\n        await browser.stop()\n\n# Run the async example\nasyncio.run(simple_browser_example())\n</code></pre> <p>Context Manager Usage</p> <p>For cleaner resource management, use the context manager pattern:</p> <pre><code>async def context_manager_example():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to(\"https://example.com\")\n        # The browser is automatically closed when exiting the context\n\nasyncio.run(context_manager_example())\n</code></pre>"},{"location":"deep-dive/browser-domain/#hierarchy-of-browser-implementations","title":"Hierarchy of Browser Implementations","text":"<p>The Browser domain follows a clear inheritance hierarchy that promotes code reuse while allowing for browser-specific implementations:</p> <pre><code>classDiagram\n    class Browser {\n        &lt;&lt;Abstract&gt;&gt;\n        +__init__(options, connection_port, browser_type)\n        +start()\n        +stop()\n        +get_page()\n        +connect()\n        #_get_default_binary_location()*\n    }\n\n    class Chrome {\n        +_get_default_binary_location()\n    }\n\n    class Edge {\n        +_get_default_binary_location()\n    }\n\n    Browser &lt;|-- Chrome : extends\n    Browser &lt;|-- Edge : extends</code></pre> <p>This architecture allows Pydoll to support multiple browser types through a unified interface. Each concrete implementation (Chrome, Edge) needs only to provide browser-specific details like executable discovery, while inheriting the robust core functionality from the base Browser class.</p>"},{"location":"deep-dive/browser-domain/#initialization-parameters","title":"Initialization Parameters","text":"<p>The Browser domain accepts three primary parameters during initialization, each controlling a different aspect of the browser's behavior:</p>"},{"location":"deep-dive/browser-domain/#options-parameter","title":"Options Parameter","text":"<p>The <code>options</code> parameter accepts an instance of the <code>Options</code> class that configures the browser's runtime environment:</p> <pre><code>from pydoll.browser.chrome import Chrome\nfrom pydoll.browser.options import Options\n\noptions = Options()\noptions.binary_location = '/usr/bin/google-chrome-stable'\noptions.add_argument('--headless=new')\noptions.add_argument('--disable-gpu')\noptions.add_argument('--window-size=1920,1080')\n\nbrowser = Chrome(options=options)\n</code></pre>"},{"location":"deep-dive/browser-domain/#key-option-properties","title":"Key Option Properties","text":"Property Description Example <code>binary_location</code> Path to browser executable <code>/usr/bin/chrome</code> <code>arguments</code> Command-line arguments <code>['--headless=new', '--disable-gpu']</code> <code>extensions</code> Browser extensions to load <code>['/path/to/extension.crx']</code> <p>Option Inheritance</p> <p>Each browser implementation inherits from a base <code>Options</code> class but may provide additional browser-specific options. For example, <code>ChromeOptions</code> extends the base class with Chrome-specific capabilities.</p> <pre><code>classDiagram\n    class Options {\n        +binary_location: str\n        +arguments: List[str]\n        +add_argument(arg: str)\n    }\n\n    class ChromeOptions {\n        +chrome_specific_option()\n    }\n\n    class EdgeOptions {\n        +edge_specific_option()\n    }\n\n    Options &lt;|-- ChromeOptions\n    Options &lt;|-- EdgeOptions</code></pre>"},{"location":"deep-dive/browser-domain/#connection-port-parameter","title":"Connection Port Parameter","text":"<p>The <code>connection_port</code> parameter defines which port to use for the CDP WebSocket connection:</p> <pre><code># Specify exact port for connection\nbrowser = Chrome(connection_port=9222)\n</code></pre> <p>This parameter serves two distinct purposes:</p> <ol> <li>For browser launching: Specifies which port the browser should open for CDP communication</li> <li>For connection to existing browser: Defines which port to connect to when using <code>connect()</code> instead of <code>start()</code></li> </ol> <p>Port Availability</p> <p>When not specified, Pydoll selects a random available port between 9223 and 9322. If your environment has firewall or network restrictions, you may need to explicitly set a port that's accessible.</p>"},{"location":"deep-dive/browser-domain/#browser-type-parameter","title":"Browser Type Parameter","text":"<p>The <code>browser_type</code> parameter explicitly defines which browser type is being instantiated:</p> <pre><code>from pydoll.browser.constants import BrowserType\nfrom pydoll.browser.chrome import Chrome\n\nbrowser = Chrome(browser_type=BrowserType.CHROME)\n</code></pre> <p>This parameter is primarily used internally and for specialized scenarios, as the browser type is typically inferred from the class being instantiated.</p>"},{"location":"deep-dive/browser-domain/#internal-components","title":"Internal Components","text":"<p>The Browser domain coordinates several specialized components to provide its functionality:</p>"},{"location":"deep-dive/browser-domain/#connection-handler","title":"Connection Handler","text":"<p>The ConnectionHandler establishes and maintains communication with the browser through the Chrome DevTools Protocol. It provides a layer of abstraction over the WebSocket connection, handling command execution, response processing, and event subscription.</p> <p>This component is a fundamental part of Pydoll's architecture and will be explored in more detail in the dedicated Connection Domain section.</p>"},{"location":"deep-dive/browser-domain/#browser-process-manager","title":"Browser Process Manager","text":"<p>The BrowserProcessManager handles the browser process lifecycle:</p> <pre><code>class BrowserProcessManager:\n    def start_browser_process(self, binary, port, arguments):\n        # Launch browser executable with proper arguments\n        # Monitor process startup\n        # ...\n\n    def stop_process(self):\n        # Terminate browser process\n        # Cleanup resources\n        # ...\n</code></pre> <p>This separation of concerns ensures that browser process management is decoupled from protocol communication, making the code more maintainable and testable.</p>"},{"location":"deep-dive/browser-domain/#options-manager","title":"Options Manager","text":"<p>The BrowserOptionsManager handles option validation and defaulting:</p> <pre><code>class BrowserOptionsManager:\n    @staticmethod\n    def initialize_options(options, browser_type):\n        # Create options instance if None\n        # Set appropriate defaults\n        # ...\n\n    @staticmethod\n    def add_default_arguments(options):\n        # Add required CDP arguments\n        # Configure automation settings\n        # ...\n</code></pre>"},{"location":"deep-dive/browser-domain/#proxy-manager","title":"Proxy Manager","text":"<p>The ProxyManager configures browser proxy settings:</p> <pre><code>class ProxyManager:\n    def __init__(self, options):\n        # Parse proxy settings from options\n        # ...\n\n    def get_proxy_credentials(self):\n        # Extract authentication details\n        # Format proxy configuration\n        # ...\n</code></pre> <p>This component is crucial for automated web scraping or testing scenarios that require proxy rotation or authentication.</p>"},{"location":"deep-dive/browser-domain/#lifecyle-and-context-management","title":"Lifecyle and Context Management","text":"<p>The Browser domain implements Python's asynchronous context management protocol (<code>__aenter__</code> and <code>__aexit__</code>) to provide automatic resource cleanup:</p> <pre><code>async def scrape_data():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n        await page.go_to('https://example.com')\n        # Work with page...\n        # Browser automatically closes when exiting the context\n</code></pre> <p>This pattern ensures that browser processes are properly terminated even if exceptions occur during automation, preventing resource leaks.</p>"},{"location":"deep-dive/browser-domain/#connection-modes","title":"Connection Modes","text":"<p>The Browser domain supports two primary modes of operation:</p>"},{"location":"deep-dive/browser-domain/#1-start-mode","title":"1. Start Mode","text":"<p>The <code>start()</code> method launches a new browser instance:</p> <pre><code>browser = Chrome()\nawait browser.start()\npage = await browser.get_page()\n</code></pre>"},{"location":"deep-dive/browser-domain/#2-connect-mode","title":"2. Connect Mode","text":"<p>The <code>connect()</code> method attaches to an existing browser instance:</p> <pre><code># Connect to Chrome running with --remote-debugging-port=9222\nbrowser = Chrome(connection_port=9222)\npage = await browser.connect()\n</code></pre> <p>Development Workflow Tip</p> <p>The connect mode is particularly valuable during development:</p> <pre><code># Fast development loop\nbrowser = Chrome(connection_port=9222)\n\nwhile True:\n    try:\n        page = await browser.connect()\n        # Test your automation code\n        # No need to restart browser between runs\n        break\n    except Exception as e:\n        print(f\"Error: {e}, retrying...\")\n</code></pre>"},{"location":"deep-dive/browser-domain/#page-management","title":"Page Management","text":"<p>The Browser domain provides several methods for managing browser pages:</p>"},{"location":"deep-dive/browser-domain/#creating-and-getting-pages","title":"Creating and Getting Pages","text":"<pre><code># Create a new empty page\nnew_page_id = await browser.new_page()\n\n# Create a new page and navigate to a URL\nnew_page_id = await browser.new_page(\"https://example.com\")\n\n# Get a page object from an existing or new page\npage = await browser.get_page()\n\n# Get a page object for a specific page ID\npage = await browser.get_page_by_id(page_id)\n</code></pre> <p>Multi-Page Automation</p> <p>You can work with multiple pages simultaneously:</p> <pre><code>async def multi_page_example():\n    browser = Chrome()\n    await browser.start()\n\n    # Create and work with multiple pages\n    page1 = await browser.get_page()\n    await page1.go_to(\"https://example.com\")\n\n    page2 = await browser.get_page()\n    await page2.go_to(\"https://github.com\")\n\n    # Get information from both pages\n    title1 = await page1.execute_script(\"return document.title\")\n    title2 = await page2.execute_script(\"return document.title\")\n\n    print(f\"Page 1: {title1}\")\n    print(f\"Page 2: {title2}\")\n\n    await browser.stop()\n</code></pre>"},{"location":"deep-dive/browser-domain/#listing-open-pages","title":"Listing Open Pages","text":"<p>To get information about all open pages in the browser:</p> <pre><code># Get all targets (pages, service workers, etc.)\ntargets = await browser.get_targets()\n\n# Filter for page targets only\npages = [t for t in targets if t.get('type') == 'page']\n\nfor page in pages:\n    print(f\"Page ID: {page['targetId']}\")\n    print(f\"URL: {page['url']}\")\n</code></pre>"},{"location":"deep-dive/browser-domain/#window-management","title":"Window Management","text":"<p>The Browser domain provides methods to control the browser window:</p> <pre><code># Get the current window ID\nwindow_id = await browser.get_window_id()\n\n# Set window bounds (position and size)\nawait browser.set_window_bounds({\n    'left': 100,\n    'top': 100,\n    'width': 1024,\n    'height': 768\n})\n\n# Maximize the window\nawait browser.set_window_maximized()\n\n# Minimize the window\nawait browser.set_window_minimized()\n</code></pre> <p>Window Management Use Cases</p> <p>Window management is particularly useful for: - Setting precise window sizes for consistent screenshots - Positioning windows for multi-monitor setups - Creating user-friendly automation that's visible during development</p>"},{"location":"deep-dive/browser-domain/#cookie-management","title":"Cookie Management","text":"<p>The Browser domain provides methods for browser-wide cookie management:</p> <pre><code># Set cookies at the browser level\ncookies_to_set = [\n    {\n        \"name\": \"session_id\",\n        \"value\": \"global_session_123\",\n        \"domain\": \"example.com\",\n        \"path\": \"/\",\n        \"secure\": True,\n        \"httpOnly\": True\n    }\n]\nawait browser.set_cookies(cookies_to_set)\n\n# Get all cookies from the browser\nall_cookies = await browser.get_cookies()\nprint(f\"Number of cookies: {len(all_cookies)}\")\n\n# Delete all cookies from the browser\nawait browser.delete_all_cookies()\n</code></pre> <p>Browser vs Page Cookie Management</p> <ul> <li>Browser-level cookies (using the methods above) apply to all pages in the browser</li> <li>Page-level cookies (using <code>page.set_cookies()</code>) apply only to that specific page</li> </ul> <p>Choose the appropriate scope based on your automation needs.</p>"},{"location":"deep-dive/browser-domain/#download-management","title":"Download Management","text":"<p>You can specify a download path for browser downloads:</p> <pre><code># Set a custom download path\ndownload_path = \"/path/to/downloads\"\nawait browser.set_download_path(download_path)\n\n# Navigate to a page with downloadable content\npage = await browser.get_page()\nawait page.go_to(\"https://example.com/download\")\n\n# Click a download link\ndownload_link = await page.find_element(By.ID, \"download-button\")\nawait download_link.click()\n\n# Files will be saved to the specified path\n</code></pre>"},{"location":"deep-dive/browser-domain/#event-system-overview","title":"Event System Overview","text":"<p>The Browser domain provides methods to enable and monitor various types of events. These methods include <code>enable_fetch_events()</code> and the <code>on()</code> method for registering event callbacks.</p> <p>Browser vs Page Event Scope</p> <p>When enabling events at the Browser level (e.g., <code>browser.enable_fetch_events()</code>), they apply globally to all pages in the browser. In contrast, enabling events at the Page level (e.g., <code>page.enable_fetch_events()</code>) affects only that specific page.</p> <p>This distinction is important for performance and resource management. Enable events at the browser level when you need to monitor activity across all pages, and at the page level when you only care about a specific page's events.</p> <p>Not all event types are available at both levels. For example, Page-specific events can only be enabled at the Page level, while browser-wide events can only be enabled at the Browser level.</p> <p>Detailed Event System Documentation</p> <p>The event system is a core component of Pydoll's architecture and will be covered in detail in a dedicated section. This will include event types, handling patterns, and advanced event-driven techniques.</p>"},{"location":"deep-dive/browser-domain/#proxy-configuration","title":"Proxy Configuration","text":"<p>Pydoll supports using proxies for browser connections. This is useful for web scraping, testing geo-specific content, or bypassing IP-based rate limits:</p> <pre><code>from pydoll.browser.chrome import Chrome\nfrom pydoll.browser.options import Options\n\noptions = Options()\n\n# Configure a proxy\noptions.add_argument('--proxy-server=http://proxy.example.com:8080')\n\n# For proxies requiring authentication\nbrowser = Chrome(options=options)\nawait browser.start()\n\n# Pydoll automatically handles proxy authentication challenges\npage = await browser.get_page()\nawait page.go_to(\"https://example.com\")\n</code></pre> <p>Private Proxy Authentication</p> <p>Pydoll handles private proxy authentication automatically:</p> <ol> <li>When a proxy authentication challenge is detected, Pydoll intercepts it</li> <li>The proxy credentials are applied from the options</li> <li>The authentication is completed transparently</li> <li>Your automation continues without interruption</li> </ol> <p>This makes working with authenticated proxies much simpler compared to traditional browser automation.</p>"},{"location":"deep-dive/browser-domain/#conclusion","title":"Conclusion","text":"<p>The Browser domain serves as the foundation of Pydoll's architecture, providing a powerful interface to browser instances through the Chrome DevTools Protocol. By understanding its capabilities and patterns, you can create sophisticated browser automation that's more reliable and efficient than traditional webdriver-based approaches.</p> <p>The combination of a clean abstraction layer, comprehensive event system, and direct control over the browser process enables advanced automation scenarios while maintaining a simple and intuitive API.</p>"},{"location":"deep-dive/cdp/","title":"Chrome DevTools Protocol (CDP)","text":"<p>The Chrome DevTools Protocol (CDP) is the foundation that enables Pydoll to control browsers without traditional webdrivers. Understanding how CDP works provides valuable insight into Pydoll's capabilities and internal architecture.</p>"},{"location":"deep-dive/cdp/#what-is-cdp","title":"What is CDP?","text":"<p>The Chrome DevTools Protocol is a powerful interface developed by the Chromium team that allows programmatic interaction with Chromium-based browsers. It's the same protocol used by Chrome DevTools when you inspect a webpage, but exposed as a programmable API that can be leveraged by automation tools.</p> <p>At its core, CDP provides a comprehensive set of methods and events for interfacing with browser internals. This allows for fine-grained control over every aspect of the browser, from navigating between pages to manipulating the DOM, intercepting network requests, and monitoring performance metrics.</p> <p>CDP Evolution</p> <p>The Chrome DevTools Protocol has been continuously evolving since its introduction. Google maintains and updates the protocol with each Chrome release, regularly adding new functionality and improving existing features.</p> <p>While the protocol was initially designed for Chrome's DevTools, its comprehensive capabilities have made it the foundation for next-generation browser automation tools like Puppeteer, Playwright, and of course, Pydoll.</p>"},{"location":"deep-dive/cdp/#websocket-communication","title":"WebSocket Communication","text":"<p>One of the key architectural decisions in CDP is its use of WebSockets for communication. When a Chromium-based browser is started with the remote debugging flag enabled, it opens a WebSocket server on a specified port:</p> <pre><code>chrome --remote-debugging-port=9222\n</code></pre> <p>Pydoll connects to this WebSocket endpoint to establish a bidirectional communication channel with the browser. This connection:</p> <ol> <li>Remains persistent throughout the automation session</li> <li>Enables real-time events from the browser to be pushed to the client</li> <li>Allows commands to be sent to the browser</li> <li>Supports binary data for efficient transfer of screenshots, PDFs, and other assets</li> </ol> <p>The WebSocket protocol is particularly well-suited for browser automation because it provides:</p> <ul> <li>Low latency communication - Necessary for responsive automation</li> <li>Bidirectional messaging - Essential for event-driven architecture</li> <li>Persistent connections - Eliminating connection setup overhead for each operation</li> </ul> <p>Here's a simplified view of how Pydoll's communication with the browser works:</p> <pre><code>sequenceDiagram\n    participant App as Pydoll Application\n    participant WS as WebSocket Connection\n    participant Browser as Chrome Browser\n\n    App -&gt;&gt; WS: Command: navigate to URL\n    WS -&gt;&gt; Browser: Execute navigation\n\n    Browser --&gt;&gt; WS: Send page load event\n    WS --&gt;&gt; App: Receive page load event</code></pre> <p>WebSocket vs HTTP</p> <p>Earlier browser automation protocols often relied on HTTP endpoints for communication. CDP's switch to WebSockets represents a significant architectural improvement that enables more responsive automation and real-time event monitoring.</p> <p>HTTP-based protocols require continuous polling to detect changes, creating overhead and delays. WebSockets allow the browser to push notifications to your automation script exactly when events occur, with minimal latency.</p>"},{"location":"deep-dive/cdp/#key-cdp-domains","title":"Key CDP Domains","text":"<p>CDP is organized into logical domains, each responsible for a specific aspect of browser functionality. Some of the most important domains include:</p> Domain Responsibility Example Use Cases Browser Control of the browser application itself Window management, browser context creation Page Interaction with page lifecycle Navigation, JavaScript execution, frame management DOM Access to page structure Query selectors, attribute modification, event listeners Network Network traffic monitoring and control Request interception, response examination, caching Runtime JavaScript execution environment Evaluate expressions, call functions, handle exceptions Input Simulating user interactions Mouse movements, keyboard input, touch events Target Managing browser contexts and targets Creating tabs, accessing iframes, handling popups Fetch Low-level network interception Modifying requests, simulating responses, authentication <p>Pydoll maps these CDP domains to a more intuitive API structure while preserving the full capabilities of the underlying protocol.</p>"},{"location":"deep-dive/cdp/#event-driven-architecture","title":"Event-Driven Architecture","text":"<p>One of CDP's most powerful features is its event system. The protocol allows clients to subscribe to various events that the browser emits during normal operation. These events cover virtually every aspect of browser behavior:</p> <ul> <li>Lifecycle events: Page loads, frame navigation, target creation</li> <li>DOM events: Element changes, attribute modifications</li> <li>Network events: Request/response cycles, WebSocket messages</li> <li>Execution events: JavaScript exceptions, console messages</li> <li>Performance events: Metrics for rendering, scripting, and more</li> </ul> <p>When you enable event monitoring in Pydoll (e.g., with <code>page.enable_network_events()</code>), the library sets up the necessary subscriptions with the browser and provides hooks for your code to react to these events.</p> <pre><code>from pydoll.events.network import NetworkEvents\nfrom functools import partial\n\nasync def on_request(page, event):\n    url = event['params']['request']['url']\n    print(f\"Request to: {url}\")\n\n# Subscribe to network request events\nawait page.enable_network_events()\nawait page.on(NetworkEvents.REQUEST_WILL_BE_SENT, partial(on_request, page))\n</code></pre> <p>This event-driven approach allows automation scripts to react immediately to browser state changes without relying on inefficient polling or arbitrary delays.</p>"},{"location":"deep-dive/cdp/#performance-advantages-of-direct-cdp-integration","title":"Performance Advantages of Direct CDP Integration","text":"<p>Using CDP directly, as Pydoll does, offers several performance advantages over traditional webdriver-based automation:</p>"},{"location":"deep-dive/cdp/#1-elimination-of-protocol-translation-layer","title":"1. Elimination of Protocol Translation Layer","text":"<p>Traditional webdriver-based tools like Selenium use a multi-layered approach:</p> <pre><code>graph LR\n    AS[Automation Script] --&gt; WC[WebDriver Client]\n    WC --&gt; WS[WebDriver Server]\n    WS --&gt; B[Browser]</code></pre> <p>Each layer adds overhead, especially the WebDriver server, which acts as a translation layer between the WebDriver protocol and the browser's native APIs.</p> <p>Pydoll's approach streamlines this to:</p> <pre><code>graph LR\n    AS[Automation Script] --&gt; P[Pydoll]\n    P --&gt; B[Browser via CDP]</code></pre> <p>This direct communication eliminates the computational and network overhead of the intermediate server, resulting in faster operations.</p>"},{"location":"deep-dive/cdp/#2-efficient-command-batching","title":"2. Efficient Command Batching","text":"<p>CDP allows for the batching of multiple commands in a single message, reducing the number of round trips required for complex operations. This is particularly valuable for operations that require several steps, such as finding an element and then interacting with it.</p>"},{"location":"deep-dive/cdp/#3-asynchronous-operation","title":"3. Asynchronous Operation","text":"<p>CDP's WebSocket-based, event-driven architecture aligns perfectly with Python's asyncio framework, enabling true asynchronous operation. This allows Pydoll to:</p> <ul> <li>Execute multiple operations concurrently</li> <li>Process events as they occur</li> <li>Avoid blocking the main thread during I/O operations</li> </ul> <pre><code>graph TD\n    subgraph \"Pydoll Async Architecture\"\n        EL[Event Loop]\n\n        subgraph \"Concurrent Tasks\"\n            T1[Task 1: Navigate]\n            T2[Task 2: Wait for Element]\n            T3[Task 3: Handle Network Events]\n        end\n\n        EL --&gt; T1\n        EL --&gt; T2\n        EL --&gt; T3\n\n        T1 --&gt; WS[WebSocket Connection]\n        T2 --&gt; WS\n        T3 --&gt; WS\n\n        WS --&gt; B[Browser]\n    end</code></pre> <p>Async Performance Gains</p> <p>The combination of asyncio and CDP creates a multiplicative effect on performance. In benchmark tests, Pydoll's asynchronous approach can process multiple pages in parallel with near-linear scaling, while traditional synchronous tools see diminishing returns as concurrency increases.</p> <p>For example, scraping 10 pages that each take 2 seconds to load might take over 20 seconds with a synchronous tool, but just over 2 seconds with Pydoll's async architecture (plus some minimal overhead).</p>"},{"location":"deep-dive/cdp/#4-fine-grained-control","title":"4. Fine-Grained Control","text":"<p>CDP provides more granular control over browser behavior than the WebDriver protocol. This allows Pydoll to implement optimized strategies for common operations:</p> <ul> <li>More precise waiting conditions (vs. arbitrary timeouts)</li> <li>Direct access to browser caches and storage</li> <li>Targeted JavaScript execution in specific contexts</li> <li>Detailed network control for request optimization</li> </ul>"},{"location":"deep-dive/cdp/#conclusion","title":"Conclusion","text":"<p>The Chrome DevTools Protocol forms the foundation of Pydoll's zero-webdriver approach to browser automation. By leveraging CDP's WebSocket communication, comprehensive domain coverage, event-driven architecture, and direct browser integration, Pydoll achieves superior performance and reliability compared to traditional automation tools.</p> <p>In the following sections, we'll dive deeper into how Pydoll implements specific CDP domains and transforms the low-level protocol into an intuitive, developer-friendly API. </p>"},{"location":"deep-dive/connection-layer/","title":"Connection Handler","text":"<p>The Connection Handler is the foundational layer of Pydoll's architecture, serving as the bridge between your Python code and the browser's Chrome DevTools Protocol (CDP). This component manages the WebSocket connection to the browser, handles command execution, and processes events in a non-blocking, asynchronous manner.</p> <pre><code>graph TD\n    A[Python Code] --&gt; B[Connection Handler]\n    B &lt;--&gt; C[WebSocket]\n    C &lt;--&gt; D[Browser CDP Endpoint]\n\n    subgraph \"Connection Handler\"\n        E[Command Manager]\n        F[Events Handler]\n        G[WebSocket Client]\n    end\n\n    B --&gt; E\n    B --&gt; F\n    B --&gt; G</code></pre>"},{"location":"deep-dive/connection-layer/#asynchronous-programming-model","title":"Asynchronous Programming Model","text":"<p>Pydoll is built on Python's <code>asyncio</code> framework, which enables non-blocking I/O operations. This design choice is critical for high-performance browser automation, as it allows multiple operations to occur concurrently without waiting for each to complete.</p>"},{"location":"deep-dive/connection-layer/#understanding-asyncawait","title":"Understanding Async/Await","text":"<p>To understand how async/await works in practice, let's examine a more detailed example with two concurrent operations:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\n\nasync def fetch_page_data(url):\n    print(f\"Starting fetch for {url}\")\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n\n    # Navigation takes time - this is where we yield control\n    await page.go_to(url)\n\n    # Get page title\n    title = await page.execute_script(\"return document.title\")\n\n    # Extract some data\n    description = await page.execute_script(\n        \"return document.querySelector('meta[name=\\\"description\\\"]')?.content || ''\"\n    )\n\n    await browser.stop()\n    print(f\"Completed fetch for {url}\")\n    return {\"url\": url, \"title\": title, \"description\": description}\n\nasync def main():\n    # Start two page operations concurrently\n    task1 = asyncio.create_task(fetch_page_data(\"https://example.com\"))\n    task2 = asyncio.create_task(fetch_page_data(\"https://github.com\"))\n\n    # Wait for both to complete and get results\n    result1 = await task1\n    result2 = await task2\n\n    return [result1, result2]\n\n# Run the async function\nresults = asyncio.run(main())\n</code></pre> <p>This example demonstrates how we can fetch data from two different websites concurrently, potentially cutting the overall execution time nearly in half compared to sequential execution.</p>"},{"location":"deep-dive/connection-layer/#async-execution-flow-diagram","title":"Async Execution Flow Diagram","text":"<p>Here's what happens in the event loop when executing the code above:</p> <pre><code>sequenceDiagram\n    participant A as Main Code\n    participant B as Task 1&lt;br/&gt; (example.com)\n    participant C as Task 2&lt;br/&gt; (github.com)\n    participant D as Event Loop\n\n    A-&gt;&gt;B: Create task1\n    B-&gt;&gt;D: Register in loop\n    A-&gt;&gt;C: Create task2\n    C-&gt;&gt;D: Register in loop\n    D-&gt;&gt;B: Execute until browser.start()\n    D-&gt;&gt;C: Execute until browser.start()\n    D--&gt;&gt;B: Resume after WebSocket connected\n    D--&gt;&gt;C: Resume after WebSocket connected\n    D-&gt;&gt;B: Execute until page.go_to()\n    D-&gt;&gt;C: Execute until page.go_to()\n    D--&gt;&gt;B: Resume after page loaded\n    D--&gt;&gt;C: Resume after page loaded\n    B--&gt;&gt;A: Return result\n    C--&gt;&gt;A: Return result</code></pre> <p>This sequence diagram illustrates how Python's asyncio manages the two concurrent tasks in our example code:</p> <ol> <li>The main function creates two tasks for fetching data from different websites</li> <li>Both tasks are registered in the event loop</li> <li>The event loop executes each task until it hits an <code>await</code> statement (like <code>browser.start()</code>)</li> <li>When async operations complete (like a WebSocket connection being established), tasks resume</li> <li>The loop continues to switch between tasks at each <code>await</code> point</li> <li>When each task completes, it returns its result back to the main function</li> </ol> <p>In the <code>fetch_page_data</code> example, this allows both browser instances to work concurrently - while one is waiting for a page to load, the other can be making progress. This is significantly more efficient than sequentially processing each website, as I/O wait times don't block the execution of other tasks.</p> <p>Cooperative Multitasking</p> <p>Asyncio uses cooperative multitasking, where tasks voluntarily yield control at <code>await</code> points. This differs from preemptive multitasking (threads), where tasks can be interrupted at any time. Cooperative multitasking can provide better performance for I/O-bound operations but requires careful coding to avoid blocking the event loop.</p>"},{"location":"deep-dive/connection-layer/#connection-handler-implementation","title":"Connection Handler Implementation","text":"<p>The <code>ConnectionHandler</code> class is designed to manage both command execution and event processing, providing a robust interface to the CDP WebSocket connection.</p>"},{"location":"deep-dive/connection-layer/#class-initialization","title":"Class Initialization","text":"<pre><code>def __init__(\n    self,\n    connection_port: int,\n    page_id: str = 'browser',\n    ws_address_resolver: Callable[[int], str] = get_browser_ws_address,\n    ws_connector: Callable = websockets.connect,\n):\n    # Initialize components...\n</code></pre> <p>The ConnectionHandler accepts several parameters:</p> Parameter Type Description <code>connection_port</code> <code>int</code> Port number where the browser's CDP endpoint is listening <code>page_id</code> <code>str</code> Identifier for the specific page/target (use 'browser' for browser-level connections) <code>ws_address_resolver</code> <code>Callable</code> Function to resolve the WebSocket URL from the port number <code>ws_connector</code> <code>Callable</code> Function to establish the WebSocket connection"},{"location":"deep-dive/connection-layer/#internal-components","title":"Internal Components","text":"<p>The ConnectionHandler orchestrates three primary components:</p> <ol> <li>WebSocket Connection: Manages the actual WebSocket communication with the browser</li> <li>Command Manager: Handles sending commands and receiving responses</li> <li>Events Handler: Processes events from the browser and triggers appropriate callbacks</li> </ol> <pre><code>classDiagram\n    class ConnectionHandler {\n        -_connection_port: int\n        -_page_id: str\n        -_ws_connection\n        -_command_manager: CommandManager\n        -_events_handler: EventsHandler\n        +execute_command(command, timeout) async\n        +register_callback(event_name, callback) async\n        +remove_callback(callback_id) async\n        +ping() async\n        +close() async\n        -_receive_events() async\n    }\n\n    class CommandManager {\n        -_pending_commands: dict\n        +create_command_future(command)\n        +resolve_command(id, response)\n        +remove_pending_command(id)\n    }\n\n    class EventsHandler {\n        -_callbacks: dict\n        -_network_logs: list\n        -_dialog: dict\n        +register_callback(event_name, callback, temporary)\n        +remove_callback(callback_id)\n        +clear_callbacks()\n        +process_event(event) async\n    }\n\n    ConnectionHandler *-- CommandManager\n    ConnectionHandler *-- EventsHandler</code></pre>"},{"location":"deep-dive/connection-layer/#command-execution-flow","title":"Command Execution Flow","text":"<p>When executing a command through the CDP, the ConnectionHandler follows a specific pattern:</p> <ol> <li>Ensure an active WebSocket connection exists</li> <li>Create a Future object to represent the pending response</li> <li>Send the command over the WebSocket</li> <li>Await the Future to be resolved with the response</li> <li>Return the response to the caller</li> </ol> <pre><code>async def execute_command(self, command: dict, timeout: int = 10) -&gt; dict:\n    # Validate command\n    if not isinstance(command, dict):\n        logger.error('Command must be a dictionary.')\n        raise exceptions.InvalidCommand('Command must be a dictionary')\n\n    # Ensure connection is active\n    await self._ensure_active_connection()\n\n    # Create future for this command\n    future = self._command_manager.create_command_future(command)\n    command_str = json.dumps(command)\n\n    # Send command and await response\n    try:\n        await self._ws_connection.send(command_str)\n        response: str = await asyncio.wait_for(future, timeout)\n        return json.loads(response)\n    except asyncio.TimeoutError as exc:\n        self._command_manager.remove_pending_command(command['id'])\n        raise exc\n    except websockets.ConnectionClosed as exc:\n        await self._handle_connection_loss()\n        raise exc\n</code></pre> <p>Command Timeout</p> <p>Commands that don't receive a response within the specified timeout period will raise a <code>TimeoutError</code>. This prevents automation scripts from hanging indefinitely due to missing responses. The default timeout is 10 seconds, but can be adjusted based on expected response times for complex operations.</p>"},{"location":"deep-dive/connection-layer/#event-processing-system","title":"Event Processing System","text":"<p>The event system is a key architectural component that enables reactive programming patterns in Pydoll. It allows you to register callbacks for specific browser events and have them executed automatically when those events occur.</p>"},{"location":"deep-dive/connection-layer/#event-flow","title":"Event Flow","text":"<p>The event processing flow follows these steps:</p> <ol> <li>The <code>_receive_events</code> method runs as a background task, continuously receiving messages from the WebSocket</li> <li>Each message is parsed and classified as either a command response or an event</li> <li>Events are passed to the EventsHandler for processing</li> <li>The EventsHandler identifies registered callbacks for the event and invokes them</li> </ol> <pre><code>flowchart TD\n    A[WebSocket Message] --&gt; B{Is Command Response?}\n    B --&gt;|Yes| C[Resolve Command Future]\n    B --&gt;|No| D[Process as Event]\n    D --&gt; E[Find Matching Callbacks]\n    E --&gt; F[Execute Callbacks]\n    F --&gt; G{Is Temporary?}\n    G --&gt;|Yes| H[Remove Callback]\n    G --&gt;|No| I[Keep Callback]</code></pre>"},{"location":"deep-dive/connection-layer/#callback-registration","title":"Callback Registration","text":"<p>The ConnectionHandler provides methods to register, remove, and manage event callbacks:</p> <pre><code># Register a callback for a specific event\ncallback_id = await connection.register_callback(\n    'Page.loadEventFired', \n    handle_page_load\n)\n\n# Remove a specific callback\nawait connection.remove_callback(callback_id)\n\n# Remove all callbacks\nawait connection.clear_callbacks()\n</code></pre> <p>Temporary Callbacks</p> <p>You can register a callback as temporary, which means it will be automatically removed after being triggered once. This is useful for one-time events like dialog handling:</p> <pre><code>await connection.register_callback(\n    'Page.javascriptDialogOpening',\n    handle_dialog,\n    temporary=True\n)\n</code></pre>"},{"location":"deep-dive/connection-layer/#asynchronous-callback-execution","title":"Asynchronous Callback Execution","text":"<p>Callbacks can be either synchronous functions or asynchronous coroutines. The ConnectionHandler handles both types properly:</p> <pre><code># Synchronous callback\ndef synchronous_callback(event):\n    print(f\"Event received: {event['method']}\")\n\n# Asynchronous callback\nasync def asynchronous_callback(event):\n    await asyncio.sleep(0.1)  # Perform some async operation\n    print(f\"Event processed asynchronously: {event['method']}\")\n\n# Both can be registered the same way\nawait connection.register_callback('Network.requestWillBeSent', synchronous_callback)\nawait connection.register_callback('Network.responseReceived', asynchronous_callback)\n</code></pre> <p>For asynchronous callbacks, the ConnectionHandler wraps them in a task that runs in the background, allowing the event processing loop to continue without waiting for the callback to complete.</p>"},{"location":"deep-dive/connection-layer/#connection-management","title":"Connection Management","text":"<p>The ConnectionHandler implements several strategies to ensure robust connections:</p>"},{"location":"deep-dive/connection-layer/#lazy-connection-establishment","title":"Lazy Connection Establishment","text":"<p>Connections are established only when needed, typically when the first command is executed or when explicitly requested. This lazy initialization approach conserves resources and allows for more flexible connection management.</p>"},{"location":"deep-dive/connection-layer/#automatic-reconnection","title":"Automatic Reconnection","text":"<p>If the WebSocket connection is lost or closed unexpectedly, the ConnectionHandler will attempt to re-establish it automatically when the next command is executed. This provides resilience against transient network issues.</p> <pre><code>async def _ensure_active_connection(self):\n    \"\"\"\n    Guarantees that an active connection exists before proceeding.\n    \"\"\"\n    if self._ws_connection is None or self._ws_connection.closed:\n        await self._establish_new_connection()\n</code></pre>"},{"location":"deep-dive/connection-layer/#resource-cleanup","title":"Resource Cleanup","text":"<p>The ConnectionHandler implements both explicit cleanup methods and Python's asynchronous context manager protocol (<code>__aenter__</code> and <code>__aexit__</code>), ensuring resources are properly released when no longer needed:</p> <pre><code>async def close(self):\n    \"\"\"\n    Closes the WebSocket connection and clears all callbacks.\n    \"\"\"\n    await self.clear_callbacks()\n    if self._ws_connection is not None:\n        try:\n            await self._ws_connection.close()\n        except websockets.ConnectionClosed as e:\n            logger.info(f'WebSocket connection has closed: {e}')\n        logger.info('WebSocket connection closed.')\n</code></pre> <p>Context Manager Usage</p> <p>Using the ConnectionHandler as a context manager is the recommended pattern for ensuring proper resource cleanup:</p> <pre><code>async with ConnectionHandler(9222, 'browser') as connection:\n    # Work with the connection...\n    await connection.execute_command(...)\n# Connection is automatically closed when exiting the context\n</code></pre>"},{"location":"deep-dive/connection-layer/#message-processing-pipeline","title":"Message Processing Pipeline","text":"<p>The ConnectionHandler implements a sophisticated message processing pipeline that handles the continuous stream of messages from the WebSocket connection:</p> <pre><code>sequenceDiagram\n    participant WS as WebSocket\n    participant RCV as _receive_events\n    participant MSG as _process_single_message\n    participant PARSE as _parse_message\n    participant CMD as _handle_command_message\n    participant EVT as _handle_event_message\n\n    loop While connected\n        WS-&gt;&gt;RCV: message\n        RCV-&gt;&gt;MSG: raw_message\n        MSG-&gt;&gt;PARSE: raw_message\n        PARSE--&gt;&gt;MSG: parsed JSON or None\n\n        alt Is command response\n            MSG-&gt;&gt;CMD: message\n            CMD-&gt;&gt;CMD: resolve command future\n        else Is event notification\n            MSG-&gt;&gt;EVT: message\n            EVT-&gt;&gt;EVT: process event &amp; trigger callbacks\n        end\n    end</code></pre> <p>This pipeline ensures efficient processing of both command responses and asynchronous events, allowing Pydoll to maintain responsive operation even under high message volume.</p>"},{"location":"deep-dive/connection-layer/#advanced-usage","title":"Advanced Usage","text":"<p>The ConnectionHandler is usually used indirectly through the Browser and Page classes, but it can also be used directly for advanced scenarios:</p>"},{"location":"deep-dive/connection-layer/#direct-event-monitoring","title":"Direct Event Monitoring","text":"<p>For specialized use cases, you might want to bypass the higher-level APIs and directly monitor specific CDP events:</p> <pre><code>from pydoll.connection.connection import ConnectionHandler\n\nasync def monitor_network():\n    connection = ConnectionHandler(9222)\n\n    async def log_request(event):\n        url = event['params']['request']['url']\n        print(f\"Request: {url}\")\n\n    await connection.register_callback(\n        'Network.requestWillBeSent', \n        log_request\n    )\n\n    # Enable network events via CDP command\n    await connection.execute_command({\n        \"id\": 1,\n        \"method\": \"Network.enable\"\n    })\n\n    # Keep running until interrupted\n    try:\n        while True:\n            await asyncio.sleep(1)\n    finally:\n        await connection.close()\n</code></pre>"},{"location":"deep-dive/connection-layer/#custom-command-execution","title":"Custom Command Execution","text":"<p>You can execute arbitrary CDP commands directly:</p> <pre><code>async def custom_cdp_command(connection, method, params=None):\n    command = {\n        \"id\": random.randint(1, 10000),\n        \"method\": method,\n        \"params\": params or {}\n    }\n    return await connection.execute_command(command)\n\n# Example: Get document HTML without using Page class\nasync def get_html(connection):\n    result = await custom_cdp_command(\n        connection,\n        \"Runtime.evaluate\",\n        {\"expression\": \"document.documentElement.outerHTML\"}\n    )\n    return result['result']['result']['value']\n</code></pre> <p>Advanced Interface</p> <p>Direct use of the ConnectionHandler requires a deep understanding of the Chrome DevTools Protocol. For most use cases, the higher-level Browser and Page APIs provide a more intuitive and safer interface.</p>"},{"location":"deep-dive/connection-layer/#advanced-concurrency-patterns","title":"Advanced Concurrency Patterns","text":"<p>The ConnectionHandler's asynchronous design enables sophisticated concurrency patterns:</p>"},{"location":"deep-dive/connection-layer/#parallel-command-execution","title":"Parallel Command Execution","text":"<p>Execute multiple commands concurrently and wait for all results:</p> <pre><code>async def get_page_metrics(connection):\n    commands = [\n        {\"id\": 1, \"method\": \"Performance.getMetrics\"},\n        {\"id\": 2, \"method\": \"Network.getResponseBody\", \"params\": {\"requestId\": \"...\"}},\n        {\"id\": 3, \"method\": \"DOM.getDocument\"}\n    ]\n\n    results = await asyncio.gather(\n        *(connection.execute_command(cmd) for cmd in commands)\n    )\n\n    return results\n</code></pre>"},{"location":"deep-dive/connection-layer/#conclusion","title":"Conclusion","text":"<p>The ConnectionHandler serves as the foundation of Pydoll's architecture, providing a robust, efficient interface to the Chrome DevTools Protocol. By leveraging Python's asyncio framework and WebSocket communication, it enables high-performance browser automation with elegant, event-driven programming patterns.</p> <p>Understanding the ConnectionHandler's design and operation provides valuable insights into Pydoll's internal workings and offers opportunities for advanced customization and optimization in specialized scenarios.</p> <p>For most use cases, you'll interact with the ConnectionHandler indirectly through the higher-level Browser and Page APIs, which provide a more intuitive interface while leveraging the ConnectionHandler's powerful capabilities. </p>"},{"location":"deep-dive/event-system/","title":"Event System","text":"<p>The event system is a foundational component of Pydoll's architecture, providing a powerful mechanism for responding to browser activities in real-time. This asynchronous notification system enables your automation code to react to various browser events as they occur, creating dynamic and responsive interactions.</p>"},{"location":"deep-dive/event-system/#websocket-communication-and-cdp","title":"WebSocket Communication and CDP","text":"<p>At the core of Pydoll's event system is the Chrome DevTools Protocol (CDP), which provides a structured way to interact with and monitor browser activities over WebSocket connections. This bidirectional communication channel allows your code to both send commands to the browser and receive events back.</p> <pre><code>sequenceDiagram\n    participant Client as Pydoll Code\n    participant Connection as ConnectionHandler\n    participant WebSocket\n    participant Browser\n\n    Client-&gt;&gt;Connection: Register callback for event\n    Connection-&gt;&gt;Connection: Store callback in registry\n\n    Client-&gt;&gt;Connection: Enable event domain\n    Connection-&gt;&gt;WebSocket: Send CDP command to enable domain\n    WebSocket-&gt;&gt;Browser: Forward command\n    Browser--&gt;&gt;WebSocket: Acknowledge domain enabled\n    WebSocket--&gt;&gt;Connection: Forward response\n    Connection--&gt;&gt;Client: Domain enabled\n\n    Browser-&gt;&gt;WebSocket: Event occurs, sends CDP event message\n    WebSocket-&gt;&gt;Connection: Forward event message\n    Connection-&gt;&gt;Connection: Look up callbacks for this event\n    Connection-&gt;&gt;Client: Execute registered callback</code></pre>"},{"location":"deep-dive/event-system/#websocket-communication-model","title":"WebSocket Communication Model","text":"<p>The WebSocket connection between Pydoll and the browser follows this pattern:</p> <ol> <li>Connection Establishment: When the browser starts, a WebSocket server is created, and Pydoll establishes a connection to it</li> <li>Bidirectional Messaging: Both Pydoll and the browser can send messages at any time</li> <li>Message Types:</li> <li>Commands: Sent from Pydoll to the browser (e.g., navigation, DOM manipulation)</li> <li>Command Responses: Sent from the browser to Pydoll in response to commands</li> <li>Events: Sent from the browser to Pydoll when something happens (e.g., page load, network activity)</li> </ol>"},{"location":"deep-dive/event-system/#chrome-devtools-protocol-structure","title":"Chrome DevTools Protocol Structure","text":"<p>CDP organizes its functionality into domains, each responsible for a specific area of browser functionality:</p> Domain Responsibility Typical Events Page Page lifecycle Load events, navigation, dialogs Network Network activity Request/response monitoring, WebSockets DOM Document structure DOM changes, attribute modifications Fetch Request interception Request paused, authentication required Runtime JavaScript execution Console messages, exceptions Browser Browser management Window creation, tabs, contexts <p>Each domain must be explicitly enabled before it will emit events, which helps manage performance by only processing events that are actually needed.</p>"},{"location":"deep-dive/event-system/#event-domains-and-enabling","title":"Event Domains and Enabling","text":"<p>Pydoll organizes events into logical domains that correspond to the CDP domains. Each domain must be explicitly enabled before it will emit events, which is handled through specific enabling methods.</p> <pre><code># Enable page events to monitor page load, navigation, dialogs, etc.\nawait page.enable_page_events()\n\n# Enable network events to monitor requests, responses, etc.\nawait page.enable_network_events()\n\n# Enable DOM events to monitor DOM changes\nawait page.enable_dom_events()\n\n# Enable fetch events to intercept and modify requests\nawait page.enable_fetch_events()\n</code></pre> <p>Domain Ownership</p> <p>Events belong to specific domains based on their functionality. For example, page load events belong to the Page domain, while network request events belong to the Network domain. Some domains are only available at certain levels - for instance, Page events are available on the Page instance but not directly at the Browser level.</p>"},{"location":"deep-dive/event-system/#why-enabledisable-is-required","title":"Why Enable/Disable is Required","text":"<p>The explicit enable/disable pattern serves several important purposes:</p> <ol> <li>Performance Optimization: By only enabling domains you're interested in, you reduce the overhead of event processing</li> <li>Resource Management: Some event domains (like Network or DOM monitoring) can generate large volumes of events that consume memory</li> <li>Clarity of Intent: Explicit enabling makes the automation code's intentions clear and self-documenting</li> <li>Controlled Cleanup: Explicitly disabling domains ensures proper cleanup when events are no longer needed</li> </ol> <pre><code>stateDiagram-v2\n    [*] --&gt; Disabled: Initial State\n    Disabled --&gt; Enabled: enable_xxx_events()\n    Enabled --&gt; Disabled: disable_xxx_events()\n    Enabled --&gt; [*]: Page Closed\n    Disabled --&gt; [*]: Page Closed</code></pre> <p>Event Leak Prevention</p> <p>Failing to disable event domains when they're no longer needed can lead to memory leaks and performance degradation, especially in long-running automation. Always disable event domains when you're done with them, particularly for high-volume events like network monitoring.</p>"},{"location":"deep-dive/event-system/#domain-specific-enabling-methods","title":"Domain-Specific Enabling Methods","text":"<p>Different domains are enabled through specific methods on the appropriate objects:</p> Domain Enable Method Disable Method Available On Page <code>enable_page_events()</code> <code>disable_page_events()</code> Page Network <code>enable_network_events()</code> <code>disable_network_events()</code> Page DOM <code>enable_dom_events()</code> <code>disable_dom_events()</code> Page Fetch <code>enable_fetch_events()</code> <code>disable_fetch_events()</code> Page, Browser File Chooser <code>enable_intercept_file_chooser_dialog()</code> <code>disable_intercept_file_chooser_dialog()</code> Page"},{"location":"deep-dive/event-system/#registering-event-callbacks","title":"Registering Event Callbacks","text":"<p>The central method for subscribing to events is the <code>on()</code> method, available on both Page and Browser instances:</p> <pre><code>async def on(\n    self, event_name: str, callback: callable, temporary: bool = False\n) -&gt; int:\n    \"\"\"\n    Registers an event listener for the page.\n\n    Args:\n        event_name (str): The event name to listen for.\n        callback (callable): The callback function to execute when the\n            event is triggered.\n        temporary (bool): If True, the callback will be removed after it's\n            triggered once. Defaults to False.\n\n    Returns:\n        int: The ID of the registered callback.\n    \"\"\"\n</code></pre> <p>This method returns a callback ID that can be used to remove the callback later if needed.</p>"},{"location":"deep-dive/event-system/#callback-types-and-parameters","title":"Callback Types and Parameters","text":"<p>Callbacks can be either synchronous functions or asynchronous coroutines:</p> <pre><code># Synchronous callback example\ndef handle_page_load(event):\n    print(f\"Page loaded at: {time.time()}\")\n\n# Asynchronous callback example\nasync def handle_network_request(event):\n    request_url = event['params']['request']['url']\n    print(f\"Request sent to: {request_url}\")\n    # Can perform async operations here\n    await save_request_details(request_url)\n\n# Register the callbacks\nawait page.on('Page.loadEventFired', handle_page_load)\nawait page.on('Network.requestWillBeSent', handle_network_request)\n</code></pre> <p>Asynchronous Callbacks</p> <p>Using async callbacks provides greater flexibility, allowing you to perform other async operations within the callback, such as making additional CDP commands or waiting for conditions.</p>"},{"location":"deep-dive/event-system/#using-partial-for-page-access-in-callbacks","title":"Using Partial for Page Access in Callbacks","text":"<p>A powerful technique is to use <code>functools.partial</code> to pass the Page instance to your callbacks, allowing the callback to interact with the page:</p> <pre><code>from functools import partial\n\n# Define a callback that needs access to the page\nasync def handle_navigation(page, event):\n    # The callback can now use the page object\n    print(f\"Navigation occurred to: {await page.current_url}\")\n\n    # Access page methods directly\n    elements = await page.find_elements(By.TAG_NAME, \"a\")\n    print(f\"Found {len(elements)} links on the new page\")\n\n# Register with partial to bind the page parameter\nawait page.enable_page_events()\nawait page.on(PageEvents.FRAME_NAVIGATED, partial(handle_navigation, page))\n</code></pre> <p>This technique is essential when: 1. Your callback needs to interact with the page (finding elements, executing scripts) 2. You want to maintain state between events 3. You need to coordinate actions across different event types</p> <p>Why Use Partial?</p> <p>The event system only passes the event data to callbacks. Using <code>partial</code> lets you pre-configure callbacks with additional parameters (like the page object) without modifying the callback signature expected by the event system.</p>"},{"location":"deep-dive/event-system/#temporary-callbacks","title":"Temporary Callbacks","text":"<p>For events you only want to handle once, you can use the <code>temporary</code> flag:</p> <pre><code># This callback will automatically be removed after the first time it fires\nawait page.on('Page.loadEventFired', handle_first_load, temporary=True)\n</code></pre> <p>This is particularly useful for: - One-time setup operations - Waiting for a specific event before continuing - Handling the first occurrence of an event differently</p>"},{"location":"deep-dive/event-system/#event-flow-and-lifecycle","title":"Event Flow and Lifecycle","text":"<p>Understanding the event flow is crucial for effective event handling:</p> <pre><code>flowchart TD\n    A[Browser Activity] --&gt;|Generates| B[CDP Event]\n    B --&gt;|Sent via WebSocket| C[ConnectionHandler]\n    C --&gt;|Filters by Event Name| D{Registered Callbacks?}\n    D --&gt;|Yes| E[Process Event]\n    D --&gt;|No| F[Discard Event]\n    E --&gt;|For Each Callback| G[Execute Callback]\n    G --&gt;|If Temporary| H[Remove Callback]\n    G --&gt;|If Permanent| I[Retain for Future Events]</code></pre> <p>The event lifecycle follows these steps:</p> <ol> <li>Something happens in the browser (page loads, request sent, DOM changes)</li> <li>Browser generates a CDP event message</li> <li>Message is sent over WebSocket to Pydoll</li> <li>The ConnectionHandler receives the event</li> <li>ConnectionHandler checks its registry for callbacks matching the event name</li> <li>If callbacks exist, each is executed with the event data</li> <li>If a callback was registered as temporary, it's removed after execution</li> </ol>"},{"location":"deep-dive/event-system/#predefined-event-constants","title":"Predefined Event Constants","text":"<p>Pydoll provides a comprehensive set of predefined event constants in the <code>events</code> package, making it easier to reference common events without remembering exact CDP event strings:</p> <pre><code>from pydoll.events import PageEvents, NetworkEvents, DOMEvents, FetchEvents\n\n# Using predefined events\nawait page.on(PageEvents.PAGE_LOADED, handle_page_load)\nawait page.on(NetworkEvents.REQUEST_WILL_BE_SENT, handle_request)\nawait page.on(DOMEvents.DOCUMENT_UPDATED, handle_dom_update)\nawait page.on(FetchEvents.REQUEST_PAUSED, handle_fetch_intercept)\n</code></pre> <p>Custom CDP Events</p> <p>While Pydoll provides constants for common events, you can use any valid CDP event string directly. This is useful for less common events that don't have predefined constants:</p> <pre><code># Using a direct CDP event string\nawait page.on('Security.certificateError', handle_cert_error)\n</code></pre>"},{"location":"deep-dive/event-system/#common-event-types","title":"Common Event Types","text":"<p>Here are some of the most useful events for automation and scraping:</p>"},{"location":"deep-dive/event-system/#page-events","title":"Page Events","text":"Constant CDP Event Description <code>PageEvents.PAGE_LOADED</code> <code>Page.loadEventFired</code> Fired when the page load event is triggered <code>PageEvents.DOM_CONTENT_LOADED</code> <code>Page.domContentEventFired</code> Fired when DOM content has been loaded <code>PageEvents.FILE_CHOOSER_OPENED</code> <code>Page.fileChooserOpened</code> Fired when a file picker dialog is shown <code>PageEvents.JS_DIALOG_OPENING</code> <code>Page.javascriptDialogOpening</code> Fired when a JavaScript dialog is shown <code>PageEvents.FRAME_NAVIGATED</code> <code>Page.frameNavigated</code> Fired when a frame has navigated to a new URL"},{"location":"deep-dive/event-system/#network-events","title":"Network Events","text":"Constant CDP Event Description <code>NetworkEvents.REQUEST_WILL_BE_SENT</code> <code>Network.requestWillBeSent</code> Fired when a request is about to be sent <code>NetworkEvents.RESPONSE_RECEIVED</code> <code>Network.responseReceived</code> Fired when an HTTP response is received <code>NetworkEvents.LOADING_FAILED</code> <code>Network.loadingFailed</code> Fired when a request fails to load <code>NetworkEvents.LOADING_FINISHED</code> <code>Network.loadingFinished</code> Fired when a request has finished loading <code>NetworkEvents.WEBSOCKET_FRAME_SENT</code> <code>Network.webSocketFrameSent</code> Fired when a WebSocket frame is sent"},{"location":"deep-dive/event-system/#dom-events","title":"DOM Events","text":"Constant CDP Event Description <code>DOMEvents.DOCUMENT_UPDATED</code> <code>DOM.documentUpdated</code> Fired when the document is updated <code>DOMEvents.SET_CHILD_NODES</code> <code>DOM.setChildNodes</code> Fired when child nodes are set <code>DOMEvents.ATTRIBUTE_MODIFIED</code> <code>DOM.attributeModified</code> Fired when an element's attribute is modified <code>DOMEvents.ATTRIBUTE_REMOVED</code> <code>DOM.attributeRemoved</code> Fired when an element's attribute is removed"},{"location":"deep-dive/event-system/#advanced-event-patterns","title":"Advanced Event Patterns","text":""},{"location":"deep-dive/event-system/#event-driven-scraping","title":"Event-Driven Scraping","text":"<p>Events allow you to create reactive scrapers that respond to page changes in real-time:</p> <pre><code>import asyncio\nfrom functools import partial\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\nfrom pydoll.events import NetworkEvents, PageEvents\n\nasync def scrape_dynamic_content():\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n\n    # Create a data storage container\n    scraped_data = []\n    data_complete = asyncio.Event()\n\n    # Set up a callback to extract data when AJAX responses are received\n    async def extract_data_from_response(page, event):\n        if 'api/products' in event['params']['response']['url']:\n            # Extract the response body\n            request_id = event['params']['requestId']\n            body, is_base64 = await page.get_network_response_body(request_id)\n\n            # Process the data\n            products = json.loads(body)\n            for product in products:\n                scraped_data.append({\n                    'id': product['id'],\n                    'name': product['name'],\n                    'price': product['price']\n                })\n\n            print(f\"Extracted {len(products)} products\")\n\n            # If we've collected enough data, signal completion\n            if len(scraped_data) &gt;= 100:\n                data_complete.set()\n\n    # Set up navigation monitoring\n    async def handle_page_load(page, event):\n        print(f\"Page loaded: {await page.current_url}\")\n\n        # Now that the page is loaded, trigger the infinite scroll\n        await page.execute_script(\"\"\"\n            function scrollDown() {\n                window.scrollTo(0, document.body.scrollHeight);\n                setTimeout(scrollDown, 1000);\n            }\n            scrollDown();\n        \"\"\")\n\n    # Enable events and register callbacks\n    await page.enable_network_events()\n    await page.enable_page_events()\n    await page.on(NetworkEvents.RESPONSE_RECEIVED, partial(extract_data_from_response, page))\n    await page.on(PageEvents.PAGE_LOADED, partial(handle_page_load, page))\n\n    # Navigate to the page with dynamic content\n    await page.go_to(\"https://example.com/products\")\n\n    # Wait for data collection to complete or timeout after 60 seconds\n    try:\n        await asyncio.wait_for(data_complete.wait(), timeout=60)\n    except asyncio.TimeoutError:\n        print(\"Timeout reached, continuing with data collected so far\")\n\n    # Process the collected data\n    print(f\"Total products collected: {len(scraped_data)}\")\n\n    # Clean up\n    await browser.stop()\n\n    return scraped_data\n</code></pre>"},{"location":"deep-dive/event-system/#parallel-scraping-with-events","title":"Parallel Scraping with Events","text":"<p>Events are particularly powerful when combined with concurrent execution for maximum efficiency. Pydoll excels at managing multiple Pages (tabs) simultaneously, which is one of its greatest advantages for high-performance automation.</p>"},{"location":"deep-dive/event-system/#multiple-browser-instances-approach","title":"Multiple Browser Instances Approach","text":"<p>The first approach uses multiple browser instances for completely isolated scraping tasks:</p> <pre><code>import asyncio\nfrom functools import partial\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\nfrom pydoll.events import PageEvents, NetworkEvents\n\nasync def scrape_with_events(url, product_type):\n    # Create an individual scraping task\n    browser = Chrome()\n    await browser.start()\n    page = await browser.get_page()\n\n    # Data container and completion signal\n    results = []\n    scraping_done = asyncio.Event()\n\n    # Define callbacks for data extraction\n    async def product_data_handler(page, product_type, event):\n        if f'api/{product_type}' in event['params'].get('response', {}).get('url', ''):\n            request_id = event['params']['requestId']\n            body, _ = await page.get_network_response_body(request_id)\n            data = json.loads(body)\n            results.extend(data['items'])\n\n            # Check if we have all the data we need\n            if len(results) &gt;= 20 or data.get('isLastPage', False):\n                scraping_done.set()\n\n    # Setup monitoring\n    await page.enable_network_events()\n    await page.on(\n        NetworkEvents.RESPONSE_RECEIVED, \n        partial(product_data_handler, page, product_type)\n    )\n\n    # Navigate and wait for data\n    await page.go_to(f\"{url}/{product_type}\")\n\n    try:\n        # Wait up to 30 seconds for data collection\n        await asyncio.wait_for(scraping_done.wait(), 30)\n    except asyncio.TimeoutError:\n        print(f\"Timeout for {product_type}, collected {len(results)} items\")\n\n    await browser.stop()\n    return results\n\nasync def main():\n    # Define different product categories to scrape in parallel\n    product_types = ['electronics', 'clothing', 'books', 'home']\n    base_url = 'https://example.com/products'\n\n    # Launch concurrent scraping tasks\n    tasks = [scrape_with_events(base_url, product_type) for product_type in product_types]\n    all_results = await asyncio.gather(*tasks)\n\n    # Process combined results\n    for product_type, results in zip(product_types, all_results):\n        print(f\"{product_type}: {len(results)} products found\")\n\n        # Process specific category data\n        for item in results[:3]:  # Show first 3 items\n            print(f\"  - {item['name']}: ${item['price']}\")\n\n    # Calculate overall statistics\n    total_products = sum(len(category) for category in all_results)\n    print(f\"Total products across all categories: {total_products}\")\n\n# Run the concurrent scraper\nasyncio.run(main())\n</code></pre>"},{"location":"deep-dive/event-system/#multi-tab-single-browser-approach","title":"Multi-Tab Single Browser Approach","text":"<p>A more efficient approach is to use multiple tabs within a single browser instance:</p> <pre><code>import asyncio\nfrom functools import partial\nimport json\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\nfrom pydoll.events import NetworkEvents\n\nasync def multi_tab_scraping():\n    # Create a single browser instance for all tabs\n    browser = Chrome()\n    await browser.start()\n\n    # Categories to scrape\n    categories = ['electronics', 'clothing', 'books', 'home']\n    base_url = 'https://example.com/products'\n\n    # Track results for each category\n    results = {category: [] for category in categories}\n    completion_events = {category: asyncio.Event() for category in categories}\n\n    # Create a callback for processing category data\n    async def process_category_data(page, category, event):\n        if f'api/{category}' in event['params'].get('response', {}).get('url', ''):\n            request_id = event['params']['requestId']\n            body, _ = await page.get_network_response_body(request_id)\n            data = json.loads(body)\n\n            # Add results to the appropriate category\n            results[category].extend(data['items'])\n            print(f\"Added {len(data['items'])} items to {category}, total: {len(results[category])}\")\n\n            # Signal completion if we have enough data\n            if len(results[category]) &gt;= 20 or data.get('isLastPage', False):\n                completion_events[category].set()\n\n    # Prepare pages, one for each category\n    pages = {}\n    for category in categories:\n        # Create a new tab\n        page_id = await browser.new_page()\n        page = await browser.get_page_by_id(page_id)\n        pages[category] = page\n\n        # Setup event monitoring for this tab\n        await page.enable_network_events()\n        await page.on(\n            NetworkEvents.RESPONSE_RECEIVED,\n            partial(process_category_data, page, category)\n        )\n\n        # Start navigation (don't await here to allow parallel loading)\n        asyncio.create_task(page.go_to(f\"{base_url}/{category}\"))\n\n    # Wait for all categories to complete or timeout\n    try:\n        await asyncio.wait_for(\n            asyncio.gather(*(event.wait() for event in completion_events.values())),\n            timeout=45\n        )\n    except asyncio.TimeoutError:\n        print(\"Some categories timed out, proceeding with collected data\")\n\n    # Display results\n    total_items = 0\n    for category, items in results.items():\n        count = len(items)\n        total_items += count\n        print(f\"{category}: collected {count} items\")\n\n        # Show sample items\n        for item in items[:2]:\n            print(f\"  - {item['name']}: ${item['price']}\")\n\n    print(f\"Total items across all categories: {total_items}\")\n\n    # Clean up\n    await browser.stop()\n    return results\n\n# Run the multi-tab scraper\nasyncio.run(multi_tab_scraping())\n</code></pre>"},{"location":"deep-dive/event-system/#dynamic-tab-creation-with-events","title":"Dynamic Tab Creation with Events","text":"<p>You can even create new tabs dynamically in response to events:</p> <pre><code>import asyncio\nfrom functools import partial\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\nfrom pydoll.events import PageEvents, NetworkEvents\n\nasync def dynamic_tab_creation():\n    browser = Chrome()\n    await browser.start()\n    main_page = await browser.get_page()\n\n    # Store results from all product pages\n    all_results = []\n    # Count active tabs to know when we're done\n    active_tabs = 1  # Start with 1 (main page)\n    # Event that signals all work is complete\n    all_done = asyncio.Event()\n\n    # This callback processes product links and creates a new tab for each\n    async def process_category_links(main_page, event):\n        # Check if this is the categories response\n        if 'api/categories' not in event['params'].get('response', {}).get('url', ''):\n            return\n\n        # Extract categories from the response\n        request_id = event['params']['requestId']\n        body, _ = await main_page.get_network_response_body(request_id)\n        categories = json.loads(body)\n\n        print(f\"Found {len(categories)} categories to process\")\n        nonlocal active_tabs\n        active_tabs += len(categories)  # Update tab counter\n\n        # Create a new tab for each category\n        for category in categories:\n            # Create a new tab\n            page_id = await browser.new_page()\n            page = await browser.get_page_by_id(page_id)\n\n            # Setup a callback for this tab\n            async def process_product_data(page, category_name, event):\n                if 'api/products' not in event['params'].get('response', {}).get('url', ''):\n                    return\n\n                # Process the product data\n                request_id = event['params']['requestId']\n                body, _ = await page.get_network_response_body(request_id)\n                products = json.loads(body)\n\n                # Add to results\n                nonlocal all_results\n                all_results.extend(products)\n                print(f\"Added {len(products)} products from {category_name}\")\n\n                # Close this tab when done\n                nonlocal active_tabs\n                await page.close()\n                active_tabs -= 1\n\n                # If this was the last tab, signal completion\n                if active_tabs == 0:\n                    all_done.set()\n\n            # Enable network events on the new tab\n            await page.enable_network_events()\n            await page.on(\n                NetworkEvents.RESPONSE_RECEIVED,\n                partial(process_product_data, page, category['name'])\n            )\n\n            # Navigate to the category page\n            asyncio.create_task(page.go_to(f\"https://example.com/products/{category['id']}\"))\n\n    # Set up the main page to find categories\n    await main_page.enable_network_events()\n    await main_page.on(\n        NetworkEvents.RESPONSE_RECEIVED,\n        partial(process_category_links, main_page)\n    )\n\n    # Navigate to the main categories page\n    await main_page.go_to(\"https://example.com/categories\")\n\n    # Wait for all tabs to complete their work\n    try:\n        await asyncio.wait_for(all_done.wait(), timeout=60)\n    except asyncio.TimeoutError:\n        print(\"Timeout reached, continuing with data collected so far\")\n\n    # Process results\n    print(f\"Total products collected: {len(all_results)}\")\n\n    # Clean up\n    await browser.stop()\n    return all_results\n</code></pre>"},{"location":"deep-dive/event-system/#key-advantages-of-multi-tab-automation","title":"Key Advantages of Multi-Tab Automation","text":"<p>Using multiple tabs in a single browser instance offers several significant advantages:</p> <ol> <li>Resource Efficiency: A single browser instance uses fewer system resources than multiple browsers</li> <li>Shared Session: All tabs share the same session, cookies, and cache</li> <li>Reduced Startup Time: Opening new tabs is much faster than starting new browser instances</li> <li>Dynamic Workflows: Create tabs in response to discoveries on other tabs</li> <li>Memory Efficiency: Better memory utilization compared to multiple browser instances</li> </ol> <p>Tab Management Best Practices</p> <ul> <li>Keep track of all tab references to avoid orphaned tabs</li> <li>Consider implementing a tab pool pattern for large-scale operations</li> <li>Close tabs when they're no longer needed to free up resources</li> <li>Use tab IDs to identify and organize tabs</li> <li>Consider adding timeouts to prevent hanging tabs</li> </ul> <p>This multi-tab approach is ideal for scenarios like: - Category-based scraping where each category needs its own context - Processing search results where each result needs detailed exploration - Following multiple user journeys simultaneously - Load balancing requests across multiple tabs to avoid rate limiting - Maintaining different user sessions in different tabs</p>"},{"location":"deep-dive/event-system/#coordinating-with-event-driven-actions","title":"Coordinating with Event-Driven Actions","text":"<p>Events can be used to coordinate actions in response to browser behavior:</p> <pre><code>async def wait_for_network_idle():\n    network_idle = asyncio.Event()\n    in_flight_requests = 0\n\n    async def track_request(event):\n        nonlocal in_flight_requests\n        in_flight_requests += 1\n\n    async def track_response(event):\n        nonlocal in_flight_requests\n        in_flight_requests -= 1\n        if in_flight_requests == 0:\n            network_idle.set()\n\n    await page.enable_network_events()\n    await page.on(NetworkEvents.REQUEST_WILL_BE_SENT, track_request)\n    await page.on(NetworkEvents.LOADING_FINISHED, track_response)\n    await page.on(NetworkEvents.LOADING_FAILED, track_response)\n\n    await network_idle.wait()\n\n    # Clean up\n    await page.disable_network_events()\n</code></pre>"},{"location":"deep-dive/event-system/#using-async-context-managers","title":"Using Async Context Managers","text":"<p>Pydoll implements context managers for some common event patterns, like file uploads:</p> <pre><code>async with page.expect_file_chooser(files=\"path/to/file.pdf\"):\n    # Trigger the file chooser dialog\n    upload_button = await page.find_element(By.ID, \"upload-button\")\n    await upload_button.click()\n    # Context manager handles waiting for and responding to the file chooser event\n</code></pre> <p>Creating Custom Context Managers</p> <p>You can create custom context managers for common event patterns in your own code:</p> <pre><code>@asynccontextmanager\nasync def wait_for_navigation():\n    navigation_complete = asyncio.Event()\n\n    async def on_navigation(event):\n        navigation_complete.set()\n\n    # Enable events if not already enabled\n    was_enabled = page.page_events_enabled\n    if not was_enabled:\n        await page.enable_page_events()\n\n    # Register temporary callback\n    await page.on(PageEvents.FRAME_NAVIGATED, on_navigation, temporary=True)\n\n    try:\n        yield\n        # Wait for navigation to complete\n        await navigation_complete.wait()\n    finally:\n        # Clean up if we enabled events\n        if not was_enabled:\n            await page.disable_page_events()\n</code></pre>"},{"location":"deep-dive/event-system/#domain-specific-event-features","title":"Domain-Specific Event Features","text":""},{"location":"deep-dive/event-system/#page-domain-events","title":"Page Domain Events","text":"<p>The Page domain provides events for page lifecycle and JavaScript dialogs:</p> <pre><code>from functools import partial\n\n# Enable page events\nawait page.enable_page_events()\n\n# Handle page load\nasync def handle_page_load(page, event):\n    print(f\"Page loaded: {await page.current_url}\")\n    # Perform actions after page load\n    await page.find_element(By.ID, \"search\").type_keys(\"pydoll\")\n\nawait page.on(PageEvents.PAGE_LOADED, partial(handle_page_load, page))\n\n# Handle JavaScript dialogs\nasync def handle_dialog(page, event):\n    if await page.has_dialog():\n        message = await page.get_dialog_message()\n        print(f\"Dialog message: {message}\")\n        await page.accept_dialog()\n\nawait page.on(PageEvents.JS_DIALOG_OPENING, partial(handle_dialog, page))\n</code></pre>"},{"location":"deep-dive/event-system/#network-domain-events-and-logging","title":"Network Domain Events and Logging","text":"<p>The Network domain provides comprehensive request monitoring and logging:</p> <pre><code>from functools import partial\n\n# Enable network events\nawait page.enable_network_events()\n\n# Monitor request activity\nasync def log_request(page, event):\n    url = event['params']['request']['url']\n    method = event['params']['request']['method']\n    print(f\"{method} request to: {url}\")\n\n    # You can trigger actions based on specific requests\n    if 'api/login' in url and method == 'POST':\n        print(\"Login request detected, waiting for response...\")\n\nawait page.on(NetworkEvents.REQUEST_WILL_BE_SENT, partial(log_request, page))\n\n# After performing actions, retrieve logs\napi_logs = await page.get_network_logs(matches=[\"api\", \"graphql\"])\n\n# Get response bodies for specific requests\njson_responses = await page.get_network_response_bodies(matches=[\"api/data\"])\n</code></pre>"},{"location":"deep-dive/event-system/#dom-events-for-structure-monitoring","title":"DOM Events for Structure Monitoring","text":"<p>The DOM domain provides events for monitoring document structure changes:</p> <pre><code>from functools import partial\n\n# Enable DOM events\nawait page.enable_dom_events()\n\n# Track attribute changes\nasync def track_attribute_change(page, event):\n    node_id = event['params']['nodeId']\n    name = event['params']['name']\n    value = event['params']['value']\n    print(f\"Attribute changed on node {node_id}: {name}={value}\")\n\n    # You can react to specific attribute changes\n    if name == 'data-status' and value == 'loaded':\n        element = await page.find_element(By.CSS_SELECTOR, f\"[data-id='{node_id}']\")\n        await element.click()\n\nawait page.on(DOMEvents.ATTRIBUTE_MODIFIED, partial(track_attribute_change, page))\n</code></pre>"},{"location":"deep-dive/event-system/#browser-level-vs-page-level-events","title":"Browser-Level vs. Page-Level Events","text":"<p>Pydoll's event system operates at both the browser and page levels, with important distinctions:</p> <pre><code>graph TD\n    Browser[Browser Instance] --&gt;|\"Global Events (e.g., Target events)\"| BrowserCallbacks[Browser-Level Callbacks]\n    Browser --&gt;|\"Creates\"| Page1[Page Instance 1]\n    Browser --&gt;|\"Creates\"| Page2[Page Instance 2]\n    Page1 --&gt;|\"Page-Specific Events\"| Page1Callbacks[Page 1 Callbacks]\n    Page2 --&gt;|\"Page-Specific Events\"| Page2Callbacks[Page 2 Callbacks]</code></pre>"},{"location":"deep-dive/event-system/#browser-level-events","title":"Browser-Level Events","text":"<p>Browser-level events operate globally across all pages:</p> <pre><code># Register a browser-level event\nawait browser.on('Target.targetCreated', handle_new_target)\n</code></pre> <p>Browser-level event domains are limited, and trying to use page-specific events will raise an exception:</p> <pre><code># This would raise an EventNotSupported exception\nawait browser.on(PageEvents.PAGE_LOADED, handle_page_load)  # Error!\n</code></pre>"},{"location":"deep-dive/event-system/#page-level-events","title":"Page-Level Events","text":"<p>Page-level events are specific to an individual page:</p> <pre><code># Get a specific page\npage = await browser.get_page()\n\n# Register a page-level event\nawait page.enable_page_events()\nawait page.on(PageEvents.PAGE_LOADED, handle_page_load)\n\n# Each page has its own event context\npage2 = await browser.get_page()\nawait page2.enable_page_events()\nawait page2.on(PageEvents.PAGE_LOADED, handle_different_page_load)\n</code></pre> <p>Domain-Specific Scope</p> <p>Not all event domains are available at both levels. For example:</p> <ul> <li>Fetch Events: Available at both browser and page levels</li> <li>Page Events: Available only at the page level</li> <li>Target Events: Available only at the browser level</li> </ul>"},{"location":"deep-dive/event-system/#performance-considerations","title":"Performance Considerations","text":""},{"location":"deep-dive/event-system/#event-system-overhead","title":"Event System Overhead","text":"<p>The event system adds overhead to browser automation, especially for high-frequency events:</p> Event Domain Typical Event Volume Performance Impact Page Low Minimal Network High Moderate to High DOM Very High High Fetch Moderate Moderate (higher if intercepting) <p>To minimize performance impact:</p> <ol> <li>Enable Only What You Need: Only enable event domains you're actively using</li> <li>Scope Appropriately: Use browser-level events only for truly browser-wide concerns</li> <li>Disable When Done: Always disable event domains when you're finished with them</li> <li>Filter Early: In callbacks, filter out irrelevant events as early as possible</li> <li>Use Temporary Callbacks: For one-time events, use the <code>temporary=True</code> flag</li> </ol>"},{"location":"deep-dive/event-system/#efficient-callback-patterns","title":"Efficient Callback Patterns","text":"<p>Write efficient callbacks to minimize overhead:</p> <pre><code># LESS EFFICIENT: Processes every request\nasync def log_all_requests(event):\n    print(f\"Request: {event['params']['request']['url']}\")\n\n# MORE EFFICIENT: Early filtering\nasync def log_api_requests(event):\n    url = event['params']['request']['url']\n    if '/api/' not in url:\n        return  # Early exit for non-API requests\n    print(f\"API Request: {url}\")\n</code></pre>"},{"location":"deep-dive/event-system/#conclusion","title":"Conclusion","text":"<p>Pydoll's event system provides a powerful mechanism for creating dynamic, responsive browser automation. By understanding the event flow, domain organization, and callback patterns, you can build sophisticated automation that reacts intelligently to browser state changes.</p> <p>The event system is particularly valuable for: - Building reactive scrapers that capture data as soon as it's available - Creating parallel automation tasks that maximize efficiency - Coordinating complex interactions that depend on browser state changes - Implementing robust error handling and retry mechanisms</p> <p>With techniques like using <code>partial</code> to bind page instances to callbacks and combining events with <code>asyncio.gather</code> for concurrent operations, you can create highly efficient and scalable automation solutions.</p>"},{"location":"deep-dive/find-elements-mixin/","title":"FindElements Mixin","text":"<p>The FindElementsMixin is a fundamental component in Pydoll's architecture that implements element location strategies using various selector types. This mixin provides the core capabilities for finding and interacting with elements in the DOM, serving as a bridge between high-level automation code and the browser's rendering engine.</p> <pre><code>graph TB\n    User[User Code] --&gt; Page[Page Class]\n    Page --&gt; Mixin[FindElementsMixin]\n    Mixin --&gt; Selectors[Selector Strategies]\n    Mixin --&gt; Wait[Wait Mechanisms]\n\n    Selectors --&gt; CSS[CSS Selectors]\n    Selectors --&gt; XPath[XPath]\n    Selectors --&gt; ID[Element ID]\n    Selectors --&gt; Other[Other Strategies]\n\n    Mixin --&gt; DOM[Browser DOM]\n    DOM --&gt; Elements[WebElements]</code></pre>"},{"location":"deep-dive/find-elements-mixin/#understanding-mixins-in-python","title":"Understanding Mixins in Python","text":"<p>In object-oriented programming, a mixin is a class that provides methods to other classes without being considered a base class. Unlike traditional inheritance where a subclass inherits from a parent class representing an \"is-a\" relationship, mixins implement a \"has-a\" capability relationship.</p> <pre><code># Example of a mixin in Python\nclass LoggerMixin:\n    def log(self, message):\n        print(f\"LOG: {message}\")\n\n    def log_error(self, error):\n        print(f\"ERROR: {error}\")\n\nclass DataProcessor(LoggerMixin):\n    def process_data(self, data):\n        self.log(\"Processing data...\")\n        # Process the data\n        self.log(\"Data processing complete\")\n</code></pre> <p>Mixins offer several advantages in complex software architecture:</p> <ol> <li>Code Reuse: The same functionality can be used by multiple unrelated classes</li> <li>Separation of Concerns: Each mixin handles a specific aspect of functionality</li> <li>Composition Over Inheritance: Avoids deep inheritance hierarchies</li> <li>Modularity: Features can be added or removed independently</li> </ol> <p>Mixin vs. Multiple Inheritance</p> <p>While Python supports multiple inheritance, mixins are a specific design pattern within that capability. A mixin is not meant to be instantiated on its own and typically doesn't maintain state. It provides methods that can be used by other classes without establishing an \"is-a\" relationship.</p>"},{"location":"deep-dive/find-elements-mixin/#the-document-object-model-dom","title":"The Document Object Model (DOM)","text":"<p>Before diving into element selection strategies, it's important to understand the DOM, which represents the structure of an HTML document as a tree of objects.</p> <pre><code>graph TD\n    A[Document] --&gt; B[html]\n    B --&gt; C[head]\n    B --&gt; D[body]\n    C --&gt; E[title]\n    D --&gt; F[div id='content']\n    F --&gt; G[h1]\n    F --&gt; H[p]\n    F --&gt; I[ul]\n    I --&gt; J[li]\n    I --&gt; K[li]\n    I --&gt; L[li]</code></pre> <p>The DOM is:</p> <ol> <li>Hierarchical: Elements nest within other elements, forming parent-child relationships</li> <li>Manipulable: JavaScript can modify the structure, content, and styling</li> <li>Queryable: Elements can be located using various selection strategies</li> <li>Event-driven: Elements can respond to user interactions and other events</li> </ol>"},{"location":"deep-dive/find-elements-mixin/#chrome-devtools-protocol-and-dom-access","title":"Chrome DevTools Protocol and DOM Access","text":"<p>Pydoll interacts with the DOM through the Chrome DevTools Protocol (CDP), which provides methods for querying and manipulating the document:</p> CDP Domain Purpose Example Commands DOM Access to document structure <code>querySelector</code>, <code>getDocument</code> Runtime JavaScript execution in page context <code>evaluate</code>, <code>callFunctionOn</code> Page Page-level operations <code>navigate</code>, <code>captureScreenshot</code> <p>The CDP allows both direct DOM manipulation through the DOM domain and JavaScript-based interaction through the Runtime domain. FindElementsMixin leverages both approaches for robust element selection.</p>"},{"location":"deep-dive/find-elements-mixin/#selector-types-and-strategies","title":"Selector Types and Strategies","text":"<p>The FindElementsMixin supports several types of selectors, each with its own strengths and use cases:</p>"},{"location":"deep-dive/find-elements-mixin/#css-selectors","title":"CSS Selectors","text":"<p>CSS selectors are the most common way to locate elements, using the same syntax as CSS stylesheets:</p> <pre><code># CSS selector examples\nawait page.find_element(By.CSS_SELECTOR, \"div.content &gt; p.intro\")\nawait page.find_element(By.CSS_SELECTOR, \"#login-form input[type='password']\")\n</code></pre> <p>CSS selectors offer:</p> <ul> <li>Familiarity: Web developers already know the syntax</li> <li>Conciseness: Often shorter than XPath</li> <li>Performance: Generally faster than XPath in most browsers</li> <li>Specificity: Can target elements based on attributes, position, and state</li> </ul> <p>Common CSS selector patterns:</p> Selector Description Example <code>#id</code> Selects by element ID <code>#username</code> <code>.class</code> Selects by class name <code>.btn-primary</code> <code>element</code> Selects by tag name <code>div</code> <code>parent &gt; child</code> Direct child <code>form &gt; input</code> <code>ancestor descendant</code> Any descendant <code>form input</code> <code>element[attr=value]</code> Attribute value <code>input[type=password]</code> <code>:nth-child(n)</code> Positional <code>li:nth-child(3)</code> <code>:not(selector)</code> Negation <code>div:not(.hidden)</code> <p>CSS Selector Efficiency</p> <p>For best performance, start your CSS selectors with an ID or a unique attribute when possible. The browser evaluates CSS selectors from right to left, so <code>#main-content .title</code> will first find all <code>.title</code> elements, then filter for those inside <code>#main-content</code>.</p>"},{"location":"deep-dive/find-elements-mixin/#xpath","title":"XPath","text":"<p>XPath is a powerful language for navigating XML/HTML documents:</p> <pre><code># XPath examples\nawait page.find_element(By.XPATH, \"//div[@id='content']/p[contains(text(), 'Welcome')]\")\nawait page.find_element(By.XPATH, \"//button[text()='Submit']\")\n</code></pre> <p>XPath offers:</p> <ul> <li>Power: Can navigate up the DOM tree (unlike CSS selectors)</li> <li>Text Content: Can select elements based on their text content</li> <li>Axes: Can select based on relationships (following, preceding, ancestor, etc.)</li> <li>Functions: Built-in functions for complex matching</li> <li>Logical Operators: Support for AND, OR and other logical operations</li> </ul>"},{"location":"deep-dive/find-elements-mixin/#basic-xpath-patterns","title":"Basic XPath Patterns","text":"Pattern Description Example <code>//tag</code> Any tag anywhere <code>//div</code> <code>/tag</code> Direct child of current node <code>/html/body</code> <code>[@attr='value']</code> Attribute condition <code>//input[@type='text']</code> <code>[text()='text']</code> Text content exactly matches <code>//button[text()='Submit']</code> <code>[contains(@attr, 'value')]</code> Attribute contains value <code>//div[contains(@class, 'item')]</code> <code>[contains(text(), 'text')]</code> Text content contains <code>//p[contains(text(), 'Welcome')]</code> <code>[position()=n]</code> Position in parent <code>//tr[position()=1]</code> <code>parent::tag</code> Parent with tag <code>//li/parent::ul</code> <code>following-sibling::tag</code> Following sibling <code>//h1/following-sibling::p</code>"},{"location":"deep-dive/find-elements-mixin/#advanced-xpath-techniques","title":"Advanced XPath Techniques","text":""},{"location":"deep-dive/find-elements-mixin/#conditional-expressions","title":"Conditional Expressions","text":"<p>XPath supports logical operators for complex conditions:</p> Operator Description Example <code>and</code> Both conditions must be true <code>//input[@type='text' and @required]</code> <code>or</code> Either condition can be true <code>//button[@type='submit' or @class='submit']</code> <code>not()</code> Negation of a condition <code>//input[not(@disabled)]</code> <code>|</code> Union operator (combines results) <code>//button | //input[@type='submit']</code> <p>Consider this HTML example:</p> <pre><code>&lt;form id=\"registration\"&gt;\n  &lt;div class=\"form-group\"&gt;\n    &lt;label for=\"username\"&gt;Username&lt;/label&gt;\n    &lt;input type=\"text\" id=\"username\" name=\"username\" required&gt;\n  &lt;/div&gt;\n  &lt;div class=\"form-group\"&gt;\n    &lt;label for=\"email\"&gt;Email&lt;/label&gt;\n    &lt;input type=\"email\" id=\"email\" name=\"email\" required&gt;\n  &lt;/div&gt;\n  &lt;div class=\"form-group\"&gt;\n    &lt;label for=\"password\"&gt;Password&lt;/label&gt;\n    &lt;input type=\"password\" id=\"password\" name=\"password\" required&gt;\n  &lt;/div&gt;\n  &lt;div class=\"actions\"&gt;\n    &lt;button type=\"reset\" class=\"btn-secondary\"&gt;Reset&lt;/button&gt;\n    &lt;button type=\"submit\" class=\"btn-primary\"&gt;Register&lt;/button&gt;\n  &lt;/div&gt;\n&lt;/form&gt;\n</code></pre> <p>To select all required text or email inputs: <pre><code>await page.find_elements(By.XPATH, \"//input[(@type='text' or @type='email') and @required]\")\n</code></pre></p> <p>To select all buttons that are either reset or have the secondary class: <pre><code>await page.find_elements(By.XPATH, \"//button[@type='reset' or contains(@class, 'secondary')]\")\n</code></pre></p> <p>To select the submit button using the union operator: <pre><code>await page.find_element(By.XPATH, \"//button[@type='submit'] | //input[@type='submit']\")\n</code></pre></p>"},{"location":"deep-dive/find-elements-mixin/#ancestor-and-other-axes","title":"Ancestor and Other Axes","text":"<p>XPath provides various axes to navigate the DOM in different directions:</p> Axis Description Example <code>ancestor::</code> All ancestors (parent, grandparent, etc.) <code>//input[@id='email']/ancestor::form</code> <code>ancestor-or-self::</code> The node itself and its ancestors <code>//div[@class='form-group']/ancestor-or-self::*[@id]</code> <code>descendant::</code> All descendants (children, grandchildren, etc.) <code>//form[@id='registration']/descendant::input</code> <code>preceding::</code> All nodes that appear before <code>//button[@type='submit']/preceding::input</code> <code>preceding-sibling::</code> Siblings that appear before <code>//button[@type='submit']/preceding-sibling::button</code> <code>following::</code> All nodes that appear after <code>//label[@for='username']/following::input</code> <code>following-sibling::</code> Siblings that appear after <code>//button[@type='reset']/following-sibling::button</code> <p>For example, with this HTML structure:</p> <pre><code>&lt;div id=\"products\"&gt;\n  &lt;div class=\"category\"&gt;\n    &lt;h2&gt;Electronics&lt;/h2&gt;\n    &lt;div class=\"product\" data-id=\"e1\"&gt;\n      &lt;h3&gt;Smartphone&lt;/h3&gt;\n      &lt;p class=\"price\"&gt;$599&lt;/p&gt;\n      &lt;p class=\"description\"&gt;Latest model with high-resolution camera&lt;/p&gt;\n      &lt;button class=\"add-to-cart\"&gt;Add to Cart&lt;/button&gt;\n    &lt;/div&gt;\n    &lt;div class=\"product\" data-id=\"e2\"&gt;\n      &lt;h3&gt;Laptop&lt;/h3&gt;\n      &lt;p class=\"price\"&gt;$999&lt;/p&gt;\n      &lt;p class=\"description\"&gt;Powerful processor with SSD storage&lt;/p&gt;\n      &lt;button class=\"add-to-cart\"&gt;Add to Cart&lt;/button&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div class=\"category\"&gt;\n    &lt;h2&gt;Books&lt;/h2&gt;\n    &lt;div class=\"product\" data-id=\"b1\"&gt;\n      &lt;h3&gt;Python Programming&lt;/h3&gt;\n      &lt;p class=\"price\"&gt;$39&lt;/p&gt;\n      &lt;p class=\"description\"&gt;Learn Python from basics to advanced&lt;/p&gt;\n      &lt;button class=\"add-to-cart\"&gt;Add to Cart&lt;/button&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <p>To find the category that contains a specific product: <pre><code>await page.find_element(By.XPATH, \"//div[@class='product' and contains(.,'Python Programming')]/ancestor::div[@class='category']\")\n</code></pre></p> <p>To find all products in the same category as the Python book: <pre><code>await page.find_elements(By.XPATH, \"//div[@class='product' and contains(.,'Python Programming')]/ancestor::div[@class='category']/div[@class='product']\")\n</code></pre></p> <p>To find all prices that come after the Laptop product: <pre><code>await page.find_elements(By.XPATH, \"//div[@class='product' and contains(.,'Laptop')]/following::p[@class='price']\")\n</code></pre></p>"},{"location":"deep-dive/find-elements-mixin/#chained-predicates-and-functions","title":"Chained Predicates and Functions","text":"<p>XPath allows chaining multiple predicates for precise selection:</p> <pre><code>&lt;table id=\"employees\"&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt;ID&lt;/th&gt;\n      &lt;th&gt;Name&lt;/th&gt;\n      &lt;th&gt;Department&lt;/th&gt;\n      &lt;th&gt;Salary&lt;/th&gt;\n      &lt;th&gt;Start Date&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;td&gt;1001&lt;/td&gt;\n      &lt;td&gt;John Smith&lt;/td&gt;\n      &lt;td&gt;Engineering&lt;/td&gt;\n      &lt;td&gt;75000&lt;/td&gt;\n      &lt;td&gt;2019-04-15&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;1002&lt;/td&gt;\n      &lt;td&gt;Maria Garcia&lt;/td&gt;\n      &lt;td&gt;Marketing&lt;/td&gt;\n      &lt;td&gt;70000&lt;/td&gt;\n      &lt;td&gt;2020-08-01&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;td&gt;1003&lt;/td&gt;\n      &lt;td&gt;Ahmed Khan&lt;/td&gt;\n      &lt;td&gt;Engineering&lt;/td&gt;\n      &lt;td&gt;85000&lt;/td&gt;\n      &lt;td&gt;2018-02-12&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n</code></pre> <p>To find employees in Engineering with salary &gt; 80000: <pre><code>await page.find_elements(\n    By.XPATH,\n    \"//table[@id='employees']//tr[td[3]='Engineering' and number(translate(td[4], '$,', '')) &gt; 80000]\"\n)\n</code></pre></p> <p>To find the newest employee (using date comparison): <pre><code>await page.find_element(\n    By.XPATH,\n    \"//table[@id='employees']//tr[not(//tr/td[5] &gt; td[5])]\"\n)\n</code></pre></p>"},{"location":"deep-dive/find-elements-mixin/#dynamic-content-and-variable-processing","title":"Dynamic Content and Variable Processing","text":"<p>XPath can be used to extract data from dynamic content structures:</p> <pre><code>&lt;div id=\"search-results\"&gt;\n  &lt;div class=\"result\"&gt;\n    &lt;div class=\"score\"&gt;98&lt;/div&gt;\n    &lt;h3&gt;Result Title 1&lt;/h3&gt;\n    &lt;div class=\"metadata\"&gt;\n      &lt;span class=\"author\"&gt;User123&lt;/span&gt;\n      &lt;span class=\"date\"&gt;2023-06-15&lt;/span&gt;\n      &lt;span class=\"category\"&gt;Technology&lt;/span&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;div class=\"result\"&gt;\n    &lt;div class=\"score\"&gt;75&lt;/div&gt;\n    &lt;h3&gt;Result Title 2&lt;/h3&gt;\n    &lt;div class=\"metadata\"&gt;\n      &lt;span class=\"author\"&gt;Expert99&lt;/span&gt;\n      &lt;span class=\"date\"&gt;2023-04-22&lt;/span&gt;\n      &lt;span class=\"category\"&gt;Science&lt;/span&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;!-- More results... --&gt;\n&lt;/div&gt;\n</code></pre> <p>To find results with high scores (&gt;=90) in a specific category: <pre><code>await page.find_elements(\n    By.XPATH,\n    \"//div[@class='result'][number(div[@class='score']) &gt;= 90 and .//span[@class='category']='Technology']\"\n)\n</code></pre></p> <p>To find the most recent result: <pre><code>await page.find_element(\n    By.XPATH,\n    \"//div[@class='result'][not(//div[@class='result']/div[@class='metadata']/span[@class='date'] &gt; ./div[@class='metadata']/span[@class='date'])]\"\n)\n</code></pre></p>"},{"location":"deep-dive/find-elements-mixin/#text-manipulation","title":"Text Manipulation","text":"<p>XPath provides functions to manipulate text content:</p> <pre><code>&lt;div id=\"article\"&gt;\n  &lt;h1&gt;Data Analysis with Python&lt;/h1&gt;\n  &lt;p&gt;This article discusses data analysis techniques using &lt;span class=\"code\"&gt;pandas&lt;/span&gt; and &lt;span class=\"code\"&gt;numpy&lt;/span&gt;.&lt;/p&gt;\n  &lt;div class=\"section\"&gt;\n    &lt;h2&gt;1. Data Preparation&lt;/h2&gt;\n    &lt;p&gt;Before analysis, data must be cleaned...&lt;/p&gt;\n  &lt;/div&gt;\n  &lt;div class=\"section\"&gt;\n    &lt;h2&gt;2. Exploratory Analysis&lt;/h2&gt;\n    &lt;p&gt;Explore patterns using visualization...&lt;/p&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n</code></pre> <p>To find sections with titles containing numbers: <pre><code>await page.find_elements(\n    By.XPATH,\n    \"//div[@class='section'][contains(translate(h2, '0123456789', '##########'), '#')]\"\n)\n</code></pre></p> <p>To find paragraphs mentioning Python libraries (case-insensitive): <pre><code>await page.find_elements(\n    By.XPATH,\n    \"//p[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'pandas') or contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'numpy')]\"\n)\n</code></pre></p> <p>XPath Performance</p> <p>While extremely powerful, XPath queries can be slower than CSS selectors, especially complex ones. Use CSS selectors for simple cases, and reserve XPath for scenarios that require its unique capabilities.</p> <p>XPath Testing</p> <p>When developing complex XPath expressions, test them in browser DevTools before implementing them in your automation code. In Chrome's DevTools Console, you can use <code>$x(\"your xpath here\")</code> to evaluate an XPath expression.</p>"},{"location":"deep-dive/find-elements-mixin/#other-selector-types","title":"Other Selector Types","text":"<p>In addition to CSS and XPath, FindElementsMixin supports several convenience selectors:</p> <pre><code># ID selector (shorthand for CSS #id)\nawait page.find_element(By.ID, \"username\")\n\n# Class name (shorthand for CSS .classname)\nawait page.find_element(By.CLASS_NAME, \"submit-button\")\n\n# Tag name (shorthand for CSS tagname)\nawait page.find_element(By.TAG_NAME, \"button\")\n\n# Link text (finds &lt;a&gt; elements by their text)\nawait page.find_element(By.LINK_TEXT, \"Click here\")\n\n# Partial link text\nawait page.find_element(By.PARTIAL_LINK_TEXT, \"Click\")\n\n# Name attribute (shorthand for CSS [name='value'])\nawait page.find_element(By.NAME, \"username\")\n</code></pre> <p>These specialized selectors offer:</p> <ul> <li>Simplicity: More readable for common cases</li> <li>Optimization: Some browsers optimize certain selector types internally</li> <li>Specificity: Communicate intent clearly in the code</li> </ul>"},{"location":"deep-dive/find-elements-mixin/#findelementsmixin-architecture","title":"FindElementsMixin Architecture","text":"<p>The FindElementsMixin implements element location strategies through a combination of CDP commands and appropriate waiting mechanisms:</p> <pre><code>classDiagram\n    class FindElementsMixin {\n        +find_element(by, value)\n        +find_elements(by, value)\n        +wait_element(by, value, timeout)\n        +wait_elements(by, value, timeout)\n        -_get_selector_function(by)\n        -_from_by_to_selector(by, value)\n    }\n\n    class Page {\n        -_connection_handler\n        +go_to(url)\n        +refresh()\n    }\n\n    class WebElement {\n        -_connection_handler\n        -_element_id\n        +click()\n        +type_keys(text)\n    }\n\n    Page --|&gt; FindElementsMixin : inherits\n    FindElementsMixin ..&gt; WebElement : creates</code></pre> <p>The mixin implements several key methods:</p> <ol> <li>find_element: Finds the first matching element</li> <li>find_elements: Finds all matching elements</li> <li>wait_element: Waits for an element to appear with a timeout</li> <li>wait_elements: Waits for multiple elements to appear</li> </ol>"},{"location":"deep-dive/find-elements-mixin/#selector-strategy-implementation","title":"Selector Strategy Implementation","text":"<p>Internally, the mixin converts the <code>By</code> enum and value into the appropriate CDP command:</p> <pre><code>def _from_by_to_selector(self, by: By, value: str) -&gt; str:\n    \"\"\"Converts a By enum and value to the appropriate selector string.\"\"\"\n    if by == By.ID:\n        return f'#{value}'\n    elif by == By.CLASS_NAME:\n        return f'.{value}'\n    elif by == By.TAG_NAME:\n        return value\n    # ... other conversions\n    else:\n        return value  # CSS_SELECTOR and XPATH use the value directly\n</code></pre> <p>Each search method then uses the appropriate CDP domain command:</p> <ul> <li>For CSS Selectors: <code>DOM.querySelector</code> or <code>DOM.querySelectorAll</code></li> <li>For XPath: <code>Runtime.evaluate</code> with an XPath evaluation function</li> <li>For other selectors: Converted to CSS then using DOM methods</li> </ul> <p>CDP Command Selection</p> <p>FindElementsMixin intelligently selects the most efficient CDP command based on the selector type. For example, using <code>DOM.querySelector</code> for CSS but falling back to JavaScript execution for XPath which is not directly supported by the DOM domain.</p>"},{"location":"deep-dive/find-elements-mixin/#waiting-mechanisms","title":"Waiting Mechanisms","text":"<p>A crucial aspect of web automation is handling the asynchronous nature of web pages. FindElementsMixin implements sophisticated waiting mechanisms:</p> <pre><code>async def wait_element(\n    self, by: By, value: str, timeout: int = 10, raise_exc: bool = True\n) -&gt; Optional[WebElement]:\n    \"\"\"\n    Waits for an element to be present on the page.\n\n    Args:\n        by: The method to locate the element\n        value: The selector value\n        timeout: Maximum time to wait in seconds\n        raise_exc: Whether to raise exception if element not found\n\n    Returns:\n        WebElement if found, None otherwise (if raise_exc is False)\n\n    Raises:\n        ElementNotFound: If the element is not found within timeout\n    \"\"\"\n    start_time = time.time()\n    while time.time() - start_time &lt; timeout:\n        element = await self.find_element(by, value, raise_exc=False)\n        if element:\n            return element\n        await asyncio.sleep(0.1)\n\n    if raise_exc:\n        raise ElementNotFound(f\"Element not found with {by}: {value}\")\n    return None\n</code></pre> <p>The waiting logic provides:</p> <ol> <li>Polling: Periodically checks for the element</li> <li>Timeout Management: Limits the maximum wait time</li> <li>Configurability: Allows customizing behavior when elements aren't found</li> <li>Exception Handling: Clear error messages when timeouts occur</li> </ol> <p>Effective Waiting Strategies</p> <p>When designing robust automation, choose appropriate timeouts based on:</p> <ul> <li>Page Load Speed: Slower sites need longer timeouts</li> <li>Network Conditions: Consider variability in connection speeds</li> <li>Application Behavior: Some elements appear after client-side rendering</li> <li>Critical Path: Key elements may need longer timeouts than optional ones</li> </ul>"},{"location":"deep-dive/find-elements-mixin/#element-creation","title":"Element Creation","text":"<p>Once elements are located, FindElementsMixin creates WebElement instances to represent them:</p> <pre><code>async def find_element(\n    self, by: By, value: str, raise_exc: bool = True\n) -&gt; Optional[WebElement]:\n    \"\"\"\n    Finds an element on the page using the specified locator strategy.\n\n    Args:\n        by: The method to locate the element\n        value: The selector value\n        raise_exc: Whether to raise exception if element not found\n\n    Returns:\n        WebElement if found, None otherwise (if raise_exc is False)\n\n    Raises:\n        ElementNotFound: If the element is not found and raise_exc is True\n    \"\"\"\n    selector = self._from_by_to_selector(by, value)\n    selector_function = self._get_selector_function(by)\n\n    result = await selector_function(selector)\n    if not result:\n        if raise_exc:\n            raise ElementNotFound(f\"Element not found with {by}: {value}\")\n        return None\n\n    # Create WebElement from result data\n    return WebElement(self._connection_handler, result['nodeId'], result['objectId'])\n</code></pre> <p>This pattern demonstrates the factory method design pattern, where the mixin creates WebElement instances but doesn't tightly couple to the details of how those elements work.</p>"},{"location":"deep-dive/find-elements-mixin/#selector-translation-to-cdp","title":"Selector Translation to CDP","text":"<p>Under the hood, FindElementsMixin translates high-level selector concepts into specific CDP commands:</p> Selector Type CDP Command JavaScript Fallback CSS Selector <code>DOM.querySelector</code> <code>document.querySelector()</code> XPath N/A (not directly supported) <code>document.evaluate()</code> ID <code>DOM.querySelector</code> <code>document.getElementById()</code> Class Name <code>DOM.querySelectorAll</code> <code>document.getElementsByClassName()</code> Tag Name <code>DOM.querySelectorAll</code> <code>document.getElementsByTagName()</code> <p>The mixin chooses the most efficient approach based on the capabilities of CDP and the browser:</p> <pre><code>async def _find_by_xpath(self, xpath: str):\n    \"\"\"Finds an element using XPath via JavaScript execution.\"\"\"\n    result = await self._execute_command(\n        RuntimeCommands.evaluate_script(\n            \"\"\"\n            function getElementByXpath(path) {\n              return document.evaluate(\n                path, document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null\n              ).singleNodeValue;\n            }\n            return getElementByXpath(arguments[0]);\n            \"\"\",\n            arguments=[xpath]\n        )\n    )\n    # Process result...\n</code></pre> <p>Performance Considerations</p> <p>The FindElementsMixin optimizes element location based on selector type:</p> <ul> <li>CSS selectors use direct DOM API calls when possible</li> <li>XPath requires JavaScript execution, which is slightly slower</li> <li>ID lookups are typically the fastest across all browsers</li> <li>Complex selectors may require more processing time</li> </ul>"},{"location":"deep-dive/find-elements-mixin/#conclusion","title":"Conclusion","text":"<p>The FindElementsMixin serves as a critical bridge between Pydoll's high-level automation API and the browser's DOM. By understanding its design and capabilities, you gain insight into how element location works in modern browser automation.</p> <p>The mixin showcases several important design principles:</p> <ol> <li>Separation of Concerns: Element location is isolated from other page functionality</li> <li>Abstraction: Complex CDP interactions are hidden behind a simple API</li> <li>Composability: The mixin can be used by any class needing element location</li> <li>Robustness: Built-in waiting mechanisms handle asynchronous page behavior</li> </ol> <p>When combined with the Page and WebElement domains, FindElementsMixin creates a powerful and intuitive API for browser automation that handles the complexities of DOM interaction. </p>"},{"location":"deep-dive/network-capabilities/","title":"Network Capabilities","text":"<p>Pydoll provides powerful capabilities for monitoring, intercepting, and manipulating network traffic during browser automation. These features give you fine-grained control over how your browser communicates with the web, enabling advanced use cases like request modification, response analysis, and network optimization.</p>"},{"location":"deep-dive/network-capabilities/#network-architecture-overview","title":"Network Architecture Overview","text":"<p>Pydoll's network capabilities are built on top of the Chrome DevTools Protocol (CDP), which provides a direct interface to the browser's internal networking stack. This architecture eliminates the limitations of traditional proxy-based approaches and enables real-time monitoring and modification of requests and responses.</p> <pre><code>flowchart TB\n    subgraph Browser[\"Chrome/Edge Browser\"]\n        Net[\"Network Stack\"] --&gt; CDP[\"Chrome DevTools Protocol\"]\n    end\n\n    subgraph Pydoll[\"Pydoll Library\"]\n        CDP --&gt; NetMon[\"Network Monitoring\"]\n        CDP --&gt; Interception[\"Request Interception\"]\n        CDP --&gt; Headers[\"Headers Manipulation\"]\n        CDP --&gt; Body[\"Body Modification\"]\n        CDP --&gt; Emulation[\"Network Condition Emulation\"]\n    end\n\n    subgraph UserCode[\"User Automation Code\"]\n        NetMon --&gt; Analysis[\"Traffic Analysis\"]\n        Interception --&gt; Auth[\"Authentication Handling\"]\n        Headers --&gt; CustomHeaders[\"Custom Headers Injection\"]\n        Body --&gt; DataModification[\"Request/Response Data Modification\"] \n        Emulation --&gt; Testing[\"Network Condition Testing\"]\n    end\n\n    class Browser,Pydoll,UserCode rounded\n\n\n    class Browser blue\n    class Pydoll green\n    class UserCode orange</code></pre> <p>The network capabilities in Pydoll can be organized into two main categories:</p> <ol> <li>Network Monitoring: Passive observation of network activity</li> <li>Request Interception: Active modification of network requests and responses</li> </ol>"},{"location":"deep-dive/network-capabilities/#network-monitoring","title":"Network Monitoring","text":"<p>Network monitoring allows you to observe and analyze the network activity of your browser session without modifying it. This is useful for understanding how a website loads resources, detecting API endpoints, or troubleshooting performance issues.</p>"},{"location":"deep-dive/network-capabilities/#enabling-network-monitoring","title":"Enabling Network Monitoring","text":"<p>To start monitoring network activity, you need to enable network events:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.network import NetworkEvents\nfrom functools import partial\n\nasync def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n\n        # Enable network monitoring\n        await page.enable_network_events()\n\n        # Navigate to a page\n        await page.go_to('https://example.com')\n\n        # Retrieve network logs\n        logs = await page.get_network_logs()\n        print(f\"Captured {len(logs)} network requests\")\n\nasyncio.run(main())\n</code></pre> <p>When you enable network events, Pydoll automatically captures information about all network requests, including:</p> <ul> <li>URLs</li> <li>HTTP methods</li> <li>Request headers</li> <li>Status codes</li> <li>Response sizes</li> <li>Content types</li> <li>Timing information</li> </ul>"},{"location":"deep-dive/network-capabilities/#network-event-callbacks","title":"Network Event Callbacks","text":"<p>You can register callbacks to be notified about specific network events in real-time:</p> <pre><code>from pydoll.events.network import NetworkEvents\nfrom functools import partial\n\n# Define a callback to handle request events\nasync def on_request(page, event):\n    url = event['params']['request']['url']\n    method = event['params']['request']['method']\n\n    print(f\"{method} request to: {url}\")\n\n    # You can access request headers\n    headers = event['params']['request'].get('headers', {})\n    if 'content-type' in headers:\n        print(f\"Content-Type: {headers['content-type']}\")\n\n# Define a callback to handle response events\nasync def on_response(page, event):\n    url = event['params']['response']['url']\n    status = event['params']['response']['status']\n\n    print(f\"Response from {url}: Status {status}\")\n\n    # Extract response timing information\n    timing = event['params']['response'].get('timing')\n    if timing:\n        total_time = timing['receiveHeadersEnd'] - timing['requestTime']\n        print(f\"Request completed in {total_time:.2f}s\")\n\n# Register the callbacks\nawait page.enable_network_events()\nawait page.on(NetworkEvents.REQUEST_WILL_BE_SENT, partial(on_request, page))\nawait page.on(NetworkEvents.RESPONSE_RECEIVED, partial(on_response, page))\n</code></pre>"},{"location":"deep-dive/network-capabilities/#key-network-events","title":"Key Network Events","text":"<p>Pydoll provides access to a wide range of network-related events:</p> Event Constant Description Useful Information Available <code>NetworkEvents.REQUEST_WILL_BE_SENT</code> Fired when a request is about to be sent URL, method, headers, POST data <code>NetworkEvents.RESPONSE_RECEIVED</code> Fired when HTTP response is received Status code, headers, MIME type, timing <code>NetworkEvents.LOADING_FAILED</code> Fired when a request fails Error information, canceled status <code>NetworkEvents.LOADING_FINISHED</code> Fired when a request completes Encoding, compressed data size <code>NetworkEvents.RESOURCE_CHANGED_PRIORITY</code> Fired when resource loading priority changes New priority level <code>NetworkEvents.WEBSOCKET_CREATED</code> Fired when a WebSocket is created URL, initiator <code>NetworkEvents.WEBSOCKET_FRAME_SENT</code> Fired when a WebSocket frame is sent Payload data <code>NetworkEvents.WEBSOCKET_FRAME_RECEIVED</code> Fired when a WebSocket frame is received Response data"},{"location":"deep-dive/network-capabilities/#retrieving-response-bodies","title":"Retrieving Response Bodies","text":"<p>One of the most powerful features of Pydoll's network monitoring is the ability to retrieve the actual content of responses:</p> <pre><code>import asyncio\nimport json\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.network import NetworkEvents\nfrom functools import partial\n\nasync def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n\n        # Enable network events\n        await page.enable_network_events()\n\n        # Track API requests\n        api_request_ids = []\n\n        async def track_api_request(page, event):\n            if '/api/' in event['params']['request']['url']:\n                request_id = event['params']['requestId']\n                api_request_ids.append(request_id)\n\n        await page.on(\n            NetworkEvents.REQUEST_WILL_BE_SENT, \n            partial(track_api_request, page)\n        )\n\n        # Navigate to a page with API calls\n        await page.go_to('https://example.com/data-heavy-page')\n\n        # Wait a moment for requests to complete\n        await asyncio.sleep(3)\n\n        # Fetch response bodies for API requests\n        for request_id in api_request_ids:\n            try:\n                body, is_base64 = await page.get_network_response_body(request_id)\n\n                # Parse JSON responses\n                if not is_base64 and body:\n                    try:\n                        data = json.loads(body)\n                        print(f\"API Response Data: {json.dumps(data, indent=2)[:200]}...\")\n                    except json.JSONDecodeError:\n                        print(f\"Non-JSON response: {body[:100]}...\")\n            except Exception as e:\n                print(f\"Error retrieving response for request {request_id}: {e}\")\n\n        # Alternative: use the built-in method to get responses matching a pattern\n        api_responses = await page.get_network_response_bodies(matches=['api'])\n        print(f\"Found {len(api_responses)} API responses\")\n\nasyncio.run(main())\n</code></pre> <p>The <code>get_network_response_body</code> method retrieves the complete response body for a specific request, while <code>get_network_response_bodies</code> is a convenience method that retrieves response bodies for all requests matching a URL pattern.</p>"},{"location":"deep-dive/network-capabilities/#filtering-network-logs","title":"Filtering Network Logs","text":"<p>You can filter network logs to focus on specific requests:</p> <pre><code># Get all network logs\nall_logs = await page.get_network_logs()\n\n# Filter logs by URL pattern\napi_logs = await page.get_network_logs(matches=['api'])\nimage_logs = await page.get_network_logs(matches=['.jpg', '.png', '.gif'])\n</code></pre> <p>This filtering capability is particularly useful when analyzing complex web applications with many network requests.</p>"},{"location":"deep-dive/network-capabilities/#request-interception-and-modification","title":"Request Interception and Modification","text":"<p>Request interception is where Pydoll's network capabilities truly shine. Unlike traditional browser automation tools that can only observe network traffic, Pydoll allows you to intercept and modify network requests before they are sent.</p>"},{"location":"deep-dive/network-capabilities/#the-fetch-domain","title":"The Fetch Domain","text":"<p>The Fetch domain in the Chrome DevTools Protocol provides advanced functionality for intercepting and manipulating network requests. Pydoll exposes this functionality through a clean API that makes it easy to implement complex network manipulation scenarios.</p> <pre><code>sequenceDiagram\n    participant App as Application Code\n    participant Pydoll as Pydoll Library\n    participant Browser as Browser\n    participant Server as Web Server\n\n    App-&gt;&gt;Pydoll: Enable fetch events\n    Pydoll-&gt;&gt;Browser: FetchCommands.enable_fetch_events()\n    Browser--&gt;&gt;Pydoll: Enabled\n\n    App-&gt;&gt;Pydoll: Register callback for REQUEST_PAUSED\n\n    App-&gt;&gt;Pydoll: Navigate to URL\n    Pydoll-&gt;&gt;Browser: Navigate command\n    Browser-&gt;&gt;Browser: Initiates request\n    Browser-&gt;&gt;Pydoll: Fetch.requestPaused event\n    Pydoll-&gt;&gt;App: Execute callback\n\n    App-&gt;&gt;Pydoll: Modify and continue request\n    Pydoll-&gt;&gt;Browser: FetchCommands.continue_request() with modifications\n    Browser-&gt;&gt;Server: Modified request\n\n    Server--&gt;&gt;Browser: Response\n    Browser--&gt;&gt;Pydoll: Complete\n    Pydoll--&gt;&gt;App: Continue execution</code></pre>"},{"location":"deep-dive/network-capabilities/#enabling-request-interception","title":"Enabling Request Interception","text":"<p>To intercept requests, you need to enable the Fetch domain:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.fetch import FetchEvents\nfrom pydoll.commands.fetch import FetchCommands\nfrom functools import partial\n\nasync def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n\n        # Define a request interceptor\n        async def intercept_request(page, event):\n            request_id = event['params']['requestId']\n            request = event['params']['request']\n            url = request['url']\n\n            print(f\"Intercepted request to: {url}\")\n\n            # You must continue the request to proceed\n            await page._execute_command(\n                FetchCommands.continue_request(request_id)\n            )\n\n        # Enable fetch events and register the interceptor\n        await page.enable_fetch_events()\n        await page.on(\n            FetchEvents.REQUEST_PAUSED, \n            partial(intercept_request, page)\n        )\n\n        # Navigate to a page\n        await page.go_to('https://example.com')\n\nasyncio.run(main())\n</code></pre> <p>Always Continue Intercepted Requests</p> <p>When intercepting requests, you must always call <code>FetchCommands.continue_request()</code> to proceed with the request, either with modifications or as is. If you don't, the browser will hang, waiting for a resolution of the intercepted request.</p>"},{"location":"deep-dive/network-capabilities/#interception-scope-and-resource-types","title":"Interception Scope and Resource Types","text":"<p>You can limit the scope of request interception to specific resource types:</p> <pre><code># Intercept all requests (could be resource-intensive)\nawait page.enable_fetch_events(resource_type='')\n\n# Intercept only document (HTML) requests\nawait page.enable_fetch_events(resource_type='Document')\n\n# Intercept only XHR/fetch API requests\nawait page.enable_fetch_events(resource_type='XHR')\n\n# Intercept only image requests\nawait page.enable_fetch_events(resource_type='Image')\n</code></pre> <p>Resource types available for interception:</p> Resource Type Description Common Examples <code>Document</code> Main HTML documents HTML pages, iframes <code>Stylesheet</code> CSS files .css files <code>Image</code> Image resources .jpg, .png, .gif, .webp <code>Media</code> Media files .mp4, .webm, audio files <code>Font</code> Font files .woff, .woff2, .ttf <code>Script</code> JavaScript files .js files <code>TextTrack</code> Text track files .vtt, .srt (captions, subtitles) <code>XHR</code> XMLHttpRequest calls API calls, AJAX requests <code>Fetch</code> Fetch API requests Modern API calls <code>EventSource</code> Server-sent events Stream connections <code>WebSocket</code> WebSocket connections Real-time communications <code>Manifest</code> Web app manifests .webmanifest files <code>Other</code> Other resource types Miscellaneous resources"},{"location":"deep-dive/network-capabilities/#request-modification-capabilities","title":"Request Modification Capabilities","text":"<p>When intercepting requests, you can modify various aspects of the request before it's sent to the server:</p>"},{"location":"deep-dive/network-capabilities/#1-modifying-url-and-method","title":"1. Modifying URL and Method","text":"<pre><code>async def redirect_request(page, event):\n    request_id = event['params']['requestId']\n    request = event['params']['request']\n    url = request['url']\n\n    # Redirect requests for one domain to another\n    if 'old-domain.com' in url:\n        new_url = url.replace('old-domain.com', 'new-domain.com')\n        print(f\"Redirecting {url} to {new_url}\")\n\n        await page._execute_command(\n            FetchCommands.continue_request(\n                request_id=request_id,\n                url=new_url\n            )\n        )\n    # Change GET to POST for specific endpoints\n    elif '/api/data' in url and request['method'] == 'GET':\n        print(f\"Converting GET to POST for {url}\")\n\n        await page._execute_command(\n            FetchCommands.continue_request(\n                request_id=request_id,\n                method='POST'\n            )\n        )\n    else:\n        # Continue normally\n        await page._execute_command(\n            FetchCommands.continue_request(request_id)\n        )\n</code></pre>"},{"location":"deep-dive/network-capabilities/#2-adding-or-modifying-headers","title":"2. Adding or Modifying Headers","text":"<pre><code>async def inject_headers(page, event):\n    request_id = event['params']['requestId']\n    request = event['params']['request']\n    url = request['url']\n\n    # Get existing headers\n    headers = request.get('headers', {})\n\n    # Add or modify headers\n    custom_headers = {\n        **headers,  # Keep existing headers\n        'X-Custom-Header': 'CustomValue',\n        'Authorization': 'Bearer your-token-here',\n        'User-Agent': 'Custom User Agent String',\n    }\n\n    await page._execute_command(\n        FetchCommands.continue_request(\n            request_id=request_id,\n            headers=custom_headers\n        )\n    )\n</code></pre>"},{"location":"deep-dive/network-capabilities/#3-modifying-request-body","title":"3. Modifying Request Body","text":"<pre><code>async def modify_post_data(page, event):\n    request_id = event['params']['requestId']\n    request = event['params']['request']\n    url = request['url']\n    method = request['method']\n\n    # Only process POST requests to specific endpoints\n    if method == 'POST' and '/api/submit' in url:\n        # Get the original post data, if any\n        original_post_data = request.get('postData', '{}')\n\n        try:\n            # Parse the original data\n            data = json.loads(original_post_data)\n\n            # Modify the data\n            data['additionalField'] = 'injected-value'\n            data['timestamp'] = int(time.time())\n\n            # Convert back to string\n            modified_post_data = json.dumps(data)\n\n            print(f\"Modified POST data for {url}\")\n\n            await page._execute_command(\n                FetchCommands.continue_request(\n                    request_id=request_id,\n                    post_data=modified_post_data\n                )\n            )\n        except json.JSONDecodeError:\n            # If not JSON, continue normally\n            await page._execute_command(\n                FetchCommands.continue_request(request_id)\n            )\n    else:\n        # Continue normally for non-POST requests\n        await page._execute_command(\n            FetchCommands.continue_request(request_id)\n        )\n</code></pre>"},{"location":"deep-dive/network-capabilities/#authentication-handling","title":"Authentication Handling","text":"<p>The Fetch domain can also intercept authentication challenges, allowing you to automatically handle HTTP authentication:</p> <pre><code>async def main():\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n\n        # Define authentication handler\n        async def handle_auth(page, event):\n            request_id = event['params']['requestId']\n            auth_challenge = event['params']['authChallenge']\n\n            print(f\"Authentication required: {auth_challenge['origin']}\")\n\n            # Provide credentials\n            await page._execute_command(\n                FetchCommands.continue_request_with_auth(\n                    request_id=request_id,\n                    proxy_username=\"username\",\n                    proxy_password=\"password\"\n                )\n            )\n\n        # Enable fetch events with auth handling\n        await page.enable_fetch_events(handle_auth=True)\n        await page.on(\n            FetchEvents.AUTH_REQUIRED, \n            partial(handle_auth, page)\n        )\n\n        # Navigate to a page requiring authentication\n        await page.go_to('https://protected-site.com')\n</code></pre>"},{"location":"deep-dive/network-capabilities/#advanced-network-monitoring-patterns","title":"Advanced Network Monitoring Patterns","text":""},{"location":"deep-dive/network-capabilities/#creating-a-network-activity-dashboard","title":"Creating a Network Activity Dashboard","text":"<p>You can combine network monitoring with event handling to create a real-time dashboard of network activity:</p> <pre><code>import asyncio\nimport time\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.network import NetworkEvents\nfrom functools import partial\n\nasync def main():\n    # Statistics counters\n    stats = {\n        'total_requests': 0,\n        'completed_requests': 0,\n        'failed_requests': 0,\n        'bytes_received': 0,\n        'request_types': {},\n        'status_codes': {},\n        'domains': {},\n        'start_time': time.time()\n    }\n\n    async def update_dashboard():\n        while True:\n            # Calculate elapsed time\n            elapsed = time.time() - stats['start_time']\n\n            # Clear console and print stats\n            print(\"\\033c\", end=\"\")  # Clear console\n            print(f\"Network Activity Dashboard - Running for {elapsed:.1f}s\")\n            print(f\"Total Requests: {stats['total_requests']}\")\n            print(f\"Completed: {stats['completed_requests']} | Failed: {stats['failed_requests']}\")\n            print(f\"Data Received: {stats['bytes_received'] / 1024:.1f} KB\")\n\n            print(\"\\nRequest Types:\")\n            for rtype, count in sorted(stats['request_types'].items(), key=lambda x: x[1], reverse=True):\n                print(f\"  {rtype}: {count}\")\n\n            print(\"\\nStatus Codes:\")\n            for code, count in sorted(stats['status_codes'].items()):\n                print(f\"  {code}: {count}\")\n\n            print(\"\\nTop Domains:\")\n            top_domains = sorted(stats['domains'].items(), key=lambda x: x[1], reverse=True)[:5]\n            for domain, count in top_domains:\n                print(f\"  {domain}: {count}\")\n\n            await asyncio.sleep(1)\n\n    # Start the dashboard updater task\n    dashboard_task = asyncio.create_task(update_dashboard())\n\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n\n        # Track request starts\n        async def on_request_sent(page, event):\n            stats['total_requests'] += 1\n\n            # Track request type\n            resource_type = event['params'].get('type', 'Other')\n            stats['request_types'][resource_type] = stats['request_types'].get(resource_type, 0) + 1\n\n            # Track domain\n            url = event['params']['request']['url']\n            try:\n                from urllib.parse import urlparse\n                domain = urlparse(url).netloc\n                stats['domains'][domain] = stats['domains'].get(domain, 0) + 1\n            except:\n                pass\n\n        # Track responses\n        async def on_response(page, event):\n            status = event['params']['response']['status']\n            stats['status_codes'][status] = stats['status_codes'].get(status, 0) + 1\n\n        # Track request completions\n        async def on_loading_finished(page, event):\n            stats['completed_requests'] += 1\n            if 'encodedDataLength' in event['params']:\n                stats['bytes_received'] += event['params']['encodedDataLength']\n\n        # Track failures\n        async def on_loading_failed(page, event):\n            stats['failed_requests'] += 1\n\n        # Register callbacks\n        await page.enable_network_events()\n        await page.on(NetworkEvents.REQUEST_WILL_BE_SENT, partial(on_request_sent, page))\n        await page.on(NetworkEvents.RESPONSE_RECEIVED, partial(on_response, page))\n        await page.on(NetworkEvents.LOADING_FINISHED, partial(on_loading_finished, page))\n        await page.on(NetworkEvents.LOADING_FAILED, partial(on_loading_failed, page))\n\n        # Navigate to a page with lots of requests\n        await page.go_to('https://news.ycombinator.com')\n\n        # Wait for user to press Enter to exit\n        await asyncio.sleep(60)\n\n    # Clean up\n    dashboard_task.cancel()\n\nasyncio.run(main())\n</code></pre>"},{"location":"deep-dive/network-capabilities/#api-traffic-analysis","title":"API Traffic Analysis","text":"<p>You can use network monitoring to analyze API traffic patterns:</p> <pre><code>import asyncio\nimport json\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.events.network import NetworkEvents\nfrom functools import partial\n\nasync def analyze_api_traffic():\n    api_calls = []\n\n    async with Chrome() as browser:\n        await browser.start()\n        page = await browser.get_page()\n\n        # Track API requests\n        async def track_api(page, event, request_type='request'):\n            if '/api/' in event['params']['request']['url']:\n                # Create a new tracking entry for requests\n                if request_type == 'request':\n                    request_id = event['params']['requestId']\n                    request = event['params']['request']\n\n                    api_calls.append({\n                        'id': request_id,\n                        'url': request['url'],\n                        'method': request['method'],\n                        'timestamp': event['params'].get('timestamp', 0),\n                        'headers': request.get('headers', {}),\n                        'postData': request.get('postData'),\n                        'status': None,\n                        'responseHeaders': None,\n                        'responseBody': None,\n                        'duration': None\n                    })\n\n                # Update existing entries with response data\n                elif request_type == 'response':\n                    request_id = event['params']['requestId']\n                    response = event['params']['response']\n\n                    # Find the matching request\n                    for call in api_calls:\n                        if call['id'] == request_id:\n                            call['status'] = response['status']\n                            call['responseHeaders'] = response.get('headers', {})\n\n                            # Calculate request duration if timing info is available\n                            if 'timing' in response:\n                                start = response['timing'].get('requestTime', 0) * 1000\n                                end = response['timing'].get('receiveHeadersEnd', 0)\n                                call['duration'] = end - start\n\n        # Register event handlers\n        await page.enable_network_events()\n        await page.on(\n            NetworkEvents.REQUEST_WILL_BE_SENT, \n            partial(track_api, page, request_type='request')\n        )\n        await page.on(\n            NetworkEvents.RESPONSE_RECEIVED, \n            partial(track_api, page, request_type='response')\n        )\n\n        # Navigate to the page and wait for activity\n        await page.go_to('https://example.com/app')\n        await asyncio.sleep(10)  # Wait for API activity\n\n        # Fetch response bodies for the API calls\n        for call in api_calls:\n            try:\n                body, is_base64 = await page.get_network_response_body(call['id'])\n                if not is_base64:\n                    try:\n                        call['responseBody'] = json.loads(body)\n                    except:\n                        call['responseBody'] = body\n            except Exception as e:\n                print(f\"Error retrieving response body: {e}\")\n\n        # Analyze the API calls\n        print(f\"Found {len(api_calls)} API calls\")\n\n        # Group by endpoint\n        endpoints = {}\n        for call in api_calls:\n            url = call['url']\n            endpoint = url.split('/api/')[1].split('?')[0]  # Extract endpoint path\n\n            if endpoint not in endpoints:\n                endpoints[endpoint] = []\n            endpoints[endpoint].append(call)\n\n        # Print summary of endpoints\n        for endpoint, calls in endpoints.items():\n            avg_time = sum(c['duration'] for c in calls if c['duration']) / len(calls) if calls else 0\n            success_rate = sum(1 for c in calls if 200 &lt;= c.get('status', 0) &lt; 300) / len(calls) if calls else 0\n\n            print(f\"\\nEndpoint: /api/{endpoint}\")\n            print(f\"  Calls: {len(calls)}\")\n            print(f\"  Methods: {set(c['method'] for c in calls)}\")\n            print(f\"  Avg Response Time: {avg_time:.2f}ms\")\n            print(f\"  Success Rate: {success_rate:.1%}\")\n\n        return api_calls\n\n# Run the analysis\nasyncio.run(analyze_api_traffic())\n</code></pre>"},{"location":"deep-dive/network-capabilities/#performance-considerations","title":"Performance Considerations","text":"<p>While Pydoll's network capabilities are powerful, there are some performance considerations to keep in mind:</p> <ol> <li> <p>Selective Interception: Intercepting all requests can significantly slow down page loading. Be selective about which resource types you intercept.</p> </li> <li> <p>Memory Management: Network logs can consume a significant amount of memory for pages with many requests. Consider clearing logs periodically.</p> </li> <li> <p>Callback Efficiency: Keep your event callbacks efficient, especially for high-frequency events like network requests. Inefficient callbacks can slow down the entire automation process.</p> </li> <li> <p>Cleanup: Always disable network and fetch events when you're done using them to prevent memory leaks.</p> </li> </ol> <pre><code># Enable events only when needed\nawait page.enable_network_events()\nawait page.enable_fetch_events(resource_type='XHR')  # Only intercept XHR requests\n\n# Do your automation work...\n\n# Clean up when done\nawait page.disable_network_events()\nawait page.disable_fetch_events()\n</code></pre>"},{"location":"deep-dive/network-capabilities/#best-practices","title":"Best Practices","text":""},{"location":"deep-dive/network-capabilities/#1-use-resource-type-filtering-effectively","title":"1. Use Resource Type Filtering Effectively","text":"<pre><code># Bad: Intercept all requests (performance impact)\nawait page.enable_fetch_events(resource_type='')\n\n# Good: Only intercept the specific resource types you need\nawait page.enable_fetch_events(resource_type='XHR')  # For API calls\nawait page.enable_fetch_events(resource_type='Document')  # For main documents\n</code></pre>"},{"location":"deep-dive/network-capabilities/#2-always-continue-intercepted-requests","title":"2. Always Continue Intercepted Requests","text":"<pre><code># Always continue every intercepted request\nasync def intercept_handler(page, event):\n    request_id = event['params']['requestId']\n\n    # Make any modifications needed\n    custom_headers = { ... }\n\n    # Continue the request\n    await page._execute_command(\n        FetchCommands.continue_request(\n            request_id=request_id,\n            headers=custom_headers\n        )\n    )\n</code></pre>"},{"location":"deep-dive/network-capabilities/#3-implement-proper-error-handling","title":"3. Implement Proper Error Handling","text":"<pre><code>async def safe_network_handler(page, event):\n    try:\n        # Your interception logic here\n        request_id = event['params']['requestId']\n        # ...\n        await page._execute_command(FetchCommands.continue_request(request_id))\n    except Exception as e:\n        print(f\"Error in request handler: {e}\")\n        # Try to continue the request even if there was an error\n        try:\n            request_id = event['params']['requestId']\n            await page._execute_command(FetchCommands.continue_request(request_id))\n        except:\n            pass\n</code></pre>"},{"location":"deep-dive/network-capabilities/#4-use-partial-for-clean-callback-management","title":"4. Use Partial for Clean Callback Management","text":"<pre><code>from functools import partial\n\n# Define your handler with page object as first parameter\nasync def handle_request(page, config, event):\n    # Now you have access to both page and custom config\n\n# Register with partial to pre-bind parameters\nconfig = {\"headers_to_add\": {\"X-Custom\": \"Value\"}}\nawait page.on(\n    FetchEvents.REQUEST_PAUSED, \n    partial(handle_request, page, config)\n)\n</code></pre>"},{"location":"deep-dive/network-capabilities/#conclusion","title":"Conclusion","text":"<p>Pydoll's network capabilities provide unprecedented control over browser network traffic, enabling advanced use cases that go beyond traditional browser automation. Whether you're monitoring API calls, injecting custom headers, or modifying request data, these features can greatly enhance your automation workflows.</p> <p>By leveraging the power of the Chrome DevTools Protocol, Pydoll makes it easy to implement sophisticated network monitoring and interception patterns while maintaining high performance and reliability.</p> <p>Remember to use these capabilities responsibly and consider the performance implications of extensive network monitoring and interception in your automation scripts.</p>"},{"location":"deep-dive/page-domain/","title":"Page Domain","text":"<p>The Page domain forms the core of Pydoll's architecture, providing a comprehensive interface for controlling browser tabs and their content. This domain bridges your high-level automation code with the browser's capabilities, enabling everything from basic navigation to complex interaction patterns.</p> <pre><code>graph TB\n    User[User Code] --&gt; Page[Page Domain]\n\n    subgraph \"Core Capabilities\"\n        Page --&gt; Nav[Navigation]\n        Page --&gt; Elements[Element Operations]\n        Page --&gt; JS[JavaScript Execution]\n        Page --&gt; Events[Event System]\n        Page --&gt; State[Session Management]\n    end\n\n    Nav &amp; Elements &amp; JS --&gt; Website[Website]\n    Events &lt;--&gt; Website</code></pre>"},{"location":"deep-dive/page-domain/#technical-architecture","title":"Technical Architecture","text":"<p>The Page domain in Pydoll acts as an integration layer between your automation code and multiple Chrome DevTools Protocol (CDP) domains. It's implemented as a concrete class that integrates multiple functional capabilities through composition and inheritance.</p> <pre><code>classDiagram\n    class Page {\n        -_connection_handler: ConnectionHandler\n        -_page_events_enabled: bool\n        -_network_events_enabled: bool\n        -_fetch_events_enabled: bool\n        -_dom_events_enabled: bool\n        -_intercept_file_chooser_dialog_enabled: bool\n        -_cloudflare_captcha_callback_id: Any\n        +go_to(url: str, timeout: int)\n        +refresh()\n        +execute_script(script: str, element: WebElement)\n        +find_element(by: By, value: str)\n        +find_elements(by: By, value: str)\n        +wait_element(by: By, value: str, timeout: int)\n        +get_screenshot(path: str)\n        +print_to_pdf(path: str)\n        +enable_page_events()\n        +enable_network_events()\n        +on(event_name: str, callback: callable)\n    }\n\n    class FindElementsMixin {\n        +find_element(by: By, value: str)\n        +find_elements(by: By, value: str)\n        +wait_element(by: By, value: str, timeout: int)\n    }\n\n    class ConnectionHandler {\n        +execute_command(command: dict)\n        +register_callback(event_name: str, callback: callable)\n    }\n\n    class WebElement {\n        -_connection_handler: ConnectionHandler\n        -_element_id: str\n        -_object_id: str\n        +click()\n        +type_keys(text: str)\n        +get_attribute(name: str)\n    }\n\n    Page --|&gt; FindElementsMixin : inherits\n    Page *-- ConnectionHandler : uses\n    Page ..&gt; WebElement : creates\n    WebElement *-- ConnectionHandler : uses</code></pre> <p>The design leverages several key patterns:</p> <ol> <li>Inheritance - The Page class inherits from FindElementsMixin to gain element location capabilities</li> <li>Composition - It uses a ConnectionHandler to manage CDP communication</li> <li>Factory Method - It creates WebElement instances when finding elements in the page</li> <li>Command - It translates high-level methods into CDP commands</li> <li>Observer - It implements an event system for reacting to browser events</li> </ol>"},{"location":"deep-dive/page-domain/#cdp-integration","title":"CDP Integration","text":"<p>The Page domain integrates with multiple CDP domains to provide its functionality:</p> CDP Domain Purpose Page Core page lifecycle and navigation Runtime JavaScript execution in page context DOM Document structure and element access Network Network operations and cookie management Fetch Request interception and modification Input User interaction simulation <p>This integration creates a powerful abstraction that simplifies browser automation while providing access to the full capabilities of the underlying protocol.</p> <pre><code>sequenceDiagram\n    participant Client as User Code\n    participant Page as Page Domain\n    participant CDP as Chrome DevTools Protocol\n    participant Browser as Browser\n\n    Client-&gt;&gt;Page: await page.go_to(\"https://example.com\")\n    Page-&gt;&gt;CDP: Navigation.navigate\n    CDP-&gt;&gt;Browser: Execute navigation\n\n    Browser--&gt;&gt;CDP: Page.loadEventFired\n    CDP--&gt;&gt;Page: Event notification\n    Page--&gt;&gt;Client: Navigation completed\n\n    Client-&gt;&gt;Page: await page.find_element(By.ID, \"login\")\n    Page-&gt;&gt;CDP: DOM.querySelector\n    CDP-&gt;&gt;Browser: Execute DOM query\n    Browser--&gt;&gt;CDP: Return element\n    CDP--&gt;&gt;Page: Element response\n    Page-&gt;&gt;Page: Create WebElement\n    Page--&gt;&gt;Client: Return WebElement</code></pre>"},{"location":"deep-dive/page-domain/#initialization-and-state-management","title":"Initialization and State Management","text":"<p>The Page class is initialized with two key parameters:</p> <pre><code>def __init__(self, connection_port: int, page_id: str):\n    \"\"\"\n    Initializes the Page instance.\n\n    Args:\n        connection_port (int): The port number for the connection to the browser.\n        page_id (str): The ID of the page, obtained via the DevTools Protocol.\n    \"\"\"\n    self._connection_handler = ConnectionHandler(connection_port, page_id)\n    self._page_events_enabled = False\n    self._network_events_enabled = False\n    self._fetch_events_enabled = False\n    self._dom_events_enabled = False\n    self._intercept_file_chooser_dialog_enabled = False\n    self._cloudflare_captcha_callback_id = None\n</code></pre> <p>The Page class maintains several state flags to track which event domains are currently enabled. This state management is crucial for:</p> <ol> <li>Preventing duplicate event registrations</li> <li>Accurately reflecting the current capabilities of the page</li> <li>Enabling proper cleanup when the page is closed</li> </ol>"},{"location":"deep-dive/page-domain/#core-patterns-and-usage","title":"Core Patterns and Usage","text":"<p>The Page domain follows a consistent pattern for interaction:</p> <pre><code>import asyncio\nfrom pydoll.browser.chrome import Chrome\nfrom pydoll.constants import By\n\nasync def pydoll_example():\n    # Create a browser instance\n    browser = Chrome()\n    await browser.start()\n\n    # Get a page\n    page = await browser.get_page()\n\n    # Work with the page...\n    await page.go_to(\"https://example.com\")\n\n    # Clean up when done\n    await browser.stop()\n\n# Run your example with asyncio\nasyncio.run(pydoll_example())\n</code></pre> <p>Most examples in this documentation assume a browser and page have already been created and will be properly cleaned up.</p>"},{"location":"deep-dive/page-domain/#navigation-system","title":"Navigation System","text":"<p>The Page domain provides a fluid navigation experience through a combination of methods that abstract the complexities of browser navigation:</p> <pre><code># Navigate to a page with custom timeout\nawait page.go_to(\"https://example.com\", timeout=60)\n\n# Get the current URL\ncurrent_url = await page.current_url\nprint(f\"Current URL: {current_url}\")\n\n# Refresh the page\nawait page.refresh()\n</code></pre> <p>Advanced Navigation</p> <p>For specialized navigation scenarios, you can combine navigation with event listeners:</p> <pre><code># Listen for network requests during navigation\nawait page.enable_network_events()\nawait page.on('Network.responseReceived', handle_response)\n\n# Navigate to the page\nawait page.go_to('https://example.com')\n</code></pre> <p>Under the hood, the navigation system performs several operations:</p> <ol> <li>Sends the navigation command through the connection handler</li> <li>Monitors page load status through periodic JavaScript evaluation</li> <li>Manages timeouts to prevent infinite waits</li> <li>Handles refresh optimization if navigating to the current URL</li> </ol>"},{"location":"deep-dive/page-domain/#element-interaction","title":"Element Interaction","text":"<p>The Page domain inherits from FindElementsMixin to provide element location functionality. This mixin-based architecture allows for code reuse while maintaining separation of concerns.</p> <p>This architecture enables the finding and interacting with page elements in a straightforward and powerful way:</p> <pre><code># Navigate to a search engine\nawait page.go_to(\"https://www.google.com\")\n\n# Find the search input field\nsearch_box = await page.find_element(By.NAME, \"q\")\n\n# Type into the search box\nawait search_box.type_keys(\"Pydoll browser automation\")\n\n# Find and click the search button\nsearch_button = await page.find_element(By.NAME, \"btnK\")\nawait search_button.click()\n\n# Wait for results to load\nresults = await page.wait_element(By.CSS_SELECTOR, \"#search\")\n\n# Find all result links\nlinks = await page.find_elements(By.CSS_SELECTOR, \"a h3\")\n\n# Print the first 3 result titles\nfor i, link in enumerate(links[:3]):\n    text = await link.text\n    print(f\"Result {i+1}: {text}\")\n</code></pre> <p>Element Interaction Chain</p> <p>The typical interaction flow in Pydoll follows this pattern:</p> <pre><code># Find an element\nbutton = await page.find_element(By.ID, 'submit-button')\n\n# Interact with the element\nawait button.click()\n\n# React to the result\nconfirmation = await page.wait_element(By.CLASS_NAME, 'success-message')\n</code></pre> <p>This interaction follows a hierarchical pattern where:</p> <ol> <li>The Page domain provides methods to find elements</li> <li>These methods return WebElement instances</li> <li>WebElement instances provide specialized interaction methods</li> </ol>"},{"location":"deep-dive/page-domain/#javascript-execution","title":"JavaScript Execution","text":"<p>The JavaScript execution system in the Page domain provides two distinct execution modes:</p> <ol> <li>Global Execution: Evaluates JavaScript in the global page context</li> <li>Element Context Execution: Executes JavaScript with an element as the context</li> </ol> <p>This dual-mode architecture enables both global script execution and element-contextualized execution:</p> <pre><code># Execute JavaScript in page context\ndimensions = await page.execute_script(\"\"\"\n    return {\n        width: window.innerWidth,\n        height: window.innerHeight,\n        devicePixelRatio: window.devicePixelRatio\n    }\n\"\"\")\nprint(f\"Window dimensions: {dimensions}\")\n\n# Find an element and manipulate it with JavaScript\nheading = await page.find_element(By.TAG_NAME, \"h1\")\n\n# Execute JavaScript with the element as context\nawait page.execute_script(\"\"\"\n    // 'argument' refers to the element\n    argument.style.color = 'red';\n    argument.style.fontSize = '32px';\n    argument.textContent = 'Modified by JavaScript';\n\"\"\", heading)\n</code></pre> <p>Script Execution Security</p> <p>When executing scripts, be aware of security implications:</p> <ul> <li>Scripts run with the full permissions of the page</li> <li>Input validation is crucial if script content includes user data</li> <li>Consider using element methods instead of scripts for standard operations</li> </ul> <p>The implementation transforms the provided JavaScript code and parameters to match the CDP requirements:</p> <ol> <li>For global execution: </li> <li>The script is sent directly to Runtime.evaluate</li> <li>For element context execution:</li> <li>The script is wrapped in a function</li> <li>'argument' references are replaced with 'this'</li> <li>The function is called with the element's objectId as context</li> </ol>"},{"location":"deep-dive/page-domain/#session-state","title":"Session State","text":"<p>The Page domain implements a sophisticated session state management system that works across multiple CDP domains, coordinating between the Network and Storage domains.</p> <p>This architecture enables sophisticated cookie and state management:</p> <pre><code># Set a cookie\ncookies_to_set = [\n    {\n        \"name\": \"session_id\",\n        \"value\": \"test_session_123\",\n        \"domain\": \"example.com\",\n        \"path\": \"/\",\n        \"secure\": True,\n        \"httpOnly\": True\n    }\n]\nawait page.set_cookies(cookies_to_set)\n\n# Get all cookies\nall_cookies = await page.get_cookies()\nprint(f\"Number of cookies: {len(all_cookies)}\")\n\n# Delete all cookies\nawait page.delete_all_cookies()\n</code></pre> <p>Page-Specific Cookie Management</p> <p>A powerful feature of Pydoll is the ability to control cookies at the individual Page level. This means you can manage multiple browser tabs, each with its own independent cookie state:</p> <pre><code># First page with one set of cookies (user A)\npage1 = await browser.get_page()\nawait page1.go_to(\"https://example.com\")\nawait page1.set_cookies([{\"name\": \"user\", \"value\": \"user_a\", \"domain\": \"example.com\"}])\n\n# Second page with different cookies (user B)\npage2 = await browser.get_page()\nawait page2.go_to(\"https://example.com\") \nawait page2.set_cookies([{\"name\": \"user\", \"value\": \"user_b\", \"domain\": \"example.com\"}])\n</code></pre> <p>This capability enables: - Testing user interactions between different account types - Comparing different user permission levels side-by-side - Maintaining multiple authenticated sessions simultaneously</p> <p>The implementation coordinates between multiple CDP domains to provide coherent state management:</p> <ol> <li>Network Domain: Manages browser-level cookies</li> <li>Storage Domain: Manages storage-level cookies</li> <li>Page's internal state: Tracks which types of storage are being used</li> </ol>"},{"location":"deep-dive/page-domain/#content-capture","title":"Content Capture","text":"<p>The Page domain provides flexible methods for capturing visual content:</p> <pre><code># Take a screenshot and save it to a file\nscreenshot_path = \"homepage.png\"\nawait page.get_screenshot(screenshot_path)\n\n# Get a screenshot as base64 (useful for embedding in reports)\nscreenshot_base64 = await page.get_screenshot_base64()\n\n# Export page as PDF\npdf_path = \"homepage.pdf\"\nawait page.print_to_pdf(pdf_path)\n</code></pre> <p>Supported Screenshot Formats</p> <p>Pydoll supports saving screenshots in several formats: - PNG (.png): Lossless compression, best for UI testing - JPEG (.jpg/.jpeg): Lossy compression, smaller file size</p> <p>If you attempt to use an unsupported format, Pydoll will raise an <code>InvalidFileExtension</code> exception.</p> <p>These visual capture capabilities are invaluable for: - Visual regression testing - Creating documentation - Debugging automation scripts - Archiving page content</p>"},{"location":"deep-dive/page-domain/#event-system-overview","title":"Event System Overview","text":"<p>The Page domain provides a comprehensive event system for monitoring and reacting to browser events. The main methods include:</p> <ul> <li>Event enabling: <code>enable_page_events()</code>, <code>enable_network_events()</code>, <code>enable_dom_events()</code>, and <code>enable_fetch_events()</code></li> <li>Event disabling: <code>disable_page_events()</code>, <code>disable_network_events()</code>, etc.</li> <li>Event registration: <code>on(event_name, callback, temporary=False)</code> for registering callbacks</li> </ul> <pre><code># Basic example of enabling events and registering a callback\nawait page.enable_page_events()\nawait page.on('Page.loadEventFired', handle_load_event)\n</code></pre> <p>Event Categories</p> <p>Pydoll supports several event categories, each requiring explicit enabling:</p> <ul> <li>Page Events: Navigation, loading, errors, dialog handling</li> <li>Network Events: Requests, responses, WebSockets</li> <li>DOM Events: Document updates, attribute changes</li> <li>Fetch Events: Request interception and modification</li> </ul> <p>Detailed Event System Documentation</p> <p>The event system is a core component of Pydoll's architecture and will be covered in detail in a dedicated section. This will include event types, handling patterns, and event-driven programming techniques.</p>"},{"location":"deep-dive/page-domain/#advanced-capabilities-implementation","title":"Advanced Capabilities Implementation","text":""},{"location":"deep-dive/page-domain/#captcha-handling","title":"Captcha Handling","text":"<p>The Page domain provides intelligent Cloudflare captcha handling through two distinct implementation approaches:</p> <ol> <li>Context Manager: Blocks until captcha is solved</li> <li>Auto-Solve: Handles captchas without blocking in the background</li> </ol> <pre><code># Context manager approach (blocks until captcha is solved)\nasync with page.expect_and_bypass_cloudflare_captcha():\n    await page.go_to(\"https://site-with-cloudflare.com\")\n    # Continue only after captcha is solved\n\n# Background processing approach\nawait page.enable_auto_solve_cloudflare_captcha()\nawait page.go_to(\"https://another-protected-site.com\")\n# Code continues immediately, captcha solved in background\n\n# When finished with auto-solving\nawait page.disable_auto_solve_cloudflare_captcha()\n</code></pre> <p>Captcha Approach Selection</p> <p>Pydoll offers two approaches for handling captchas:</p> <p>Context Manager: Blocks until captcha is solved <pre><code>async with page.expect_and_bypass_cloudflare_captcha():\n    await page.go_to('https://protected-site.com')\n# Execution continues after captcha is solved\n</code></pre></p> <p>Background Processing: Handles captchas without blocking <pre><code>await page.enable_auto_solve_cloudflare_captcha()\nawait page.go_to('https://protected-site.com')\n# Execution continues immediately, captcha solved in background\n</code></pre></p> <p>The actual implementation uses various techniques to detect and interact with captcha elements:</p> <ol> <li>Wait for the page load event</li> <li>Look for captcha elements using configurable selectors</li> <li>Adjust the element size if needed (via JavaScript)</li> <li>Trigger the captcha interaction</li> <li>Either wait for completion (context manager) or continue (background)</li> </ol>"},{"location":"deep-dive/page-domain/#dialog-management","title":"Dialog Management","text":"<p>Pydoll implements dialog handling through a combination of event monitoring and state tracking in the connection handler.</p> <pre><code># Set up a dialog handler\nasync def handle_dialog(event):\n    if await page.has_dialog():\n        message = await page.get_dialog_message()\n        print(f\"Dialog detected: {message}\")\n        await page.accept_dialog()\n\n# Enable page events to detect dialogs\nawait page.enable_page_events()\nawait page.on('Page.javascriptDialogOpening', handle_dialog)\n\n# Trigger an alert dialog\nawait page.execute_script(\"alert('This is a test alert')\")\n</code></pre> <p>Dialog State Checking</p> <p>Always check if a dialog is present with <code>await page.has_dialog()</code> before attempting to interact with it. If no dialog is present when calling <code>get_dialog_message()</code> or <code>accept_dialog()</code>, a <code>LookupError</code> will be raised.</p>"},{"location":"deep-dive/page-domain/#file-upload","title":"File Upload","text":"<p>Pydoll implements file upload handling through a sophisticated context manager pattern that handles the setup, execution, and cleanup phases of file uploads.</p> <p>This implementation streamlines file uploading:</p> <pre><code># Path to a file to upload\nfile_path = \"document.pdf\"\n\n# Use the context manager to handle file chooser dialog\nasync with page.expect_file_chooser(files=file_path):\n    # Find and click the upload button\n    upload_button = await page.find_element(By.ID, \"upload-button\")\n    await upload_button.click()\n</code></pre> <p>File Chooser Implementation</p> <p>The file chooser context manager: 1. Enables page events if not already enabled 2. Enables file chooser dialog interception 3. Sets up a temporary callback to handle the file upload 4. Restores previous settings once the upload is complete</p>"},{"location":"deep-dive/page-domain/#performance","title":"Performance","text":"<p>The Page domain implements several patterns to optimize performance through selective event enabling, temporary callbacks, and proper resource cleanup.</p>"},{"location":"deep-dive/page-domain/#event-optimization","title":"Event Optimization","text":"<p>Enable only the specific event domains necessary for your current task:</p> <pre><code># GOOD: Enable only what you need\nawait page.enable_network_events()  # Only enable network events\n\n# BAD: Enabling unnecessary events creates overhead\nawait page.enable_page_events()\nawait page.enable_network_events()\nawait page.enable_dom_events()\nawait page.enable_fetch_events()\n</code></pre> <p>Each enabled event domain adds processing overhead as events are continuously monitored and processed.</p>"},{"location":"deep-dive/page-domain/#domain-relationships","title":"Domain Relationships","text":"<p>Understanding Pydoll's domain architecture helps clarify how the Page Domain fits into the library's broader ecosystem. The relationship between domains forms a natural hierarchy of responsibilities.</p> <pre><code>graph LR\n    Browser[\"Browser Domain&lt;br&gt;(Browser management)\"]\n    Page[\"Page Domain&lt;br&gt;(Page interaction)\"]\n    Element[\"WebElement Domain&lt;br&gt;(Element interaction)\"]\n\n    Browser --&gt;|\"creates and manages\"| Page\n    Page --&gt;|\"locates and creates\"| Element</code></pre> <p>The Browser Domain sits at the top of the hierarchy, responsible for browser lifecycle, connection management, and global configuration. It serves as the initial entry point for any automation with Pydoll. This domain creates and manages page instances, providing the necessary environment for navigation.</p> <p>The Page Domain acts as the crucial intermediary, operating within the context of a specific browser. It exposes methods for navigation, content interaction, JavaScript execution, and event handling. A fundamental aspect is its ability to locate elements within the page and create WebElement instances.</p> <p>The WebElement Domain represents specific DOM elements. Each WebElement belongs to a page and provides specialized methods for interactions such as clicking, typing, or retrieving properties. While WebElements are separate entities, they always operate within the context of the page that created them.</p> <p>This layered architecture provides several benefits:</p> <ul> <li>Separation of Concerns: Each domain has a clear, well-defined purpose</li> <li>Reusability: Components can be used independently when needed</li> <li>Ease of Use: The API follows a natural flow from browser \u2192 page \u2192 element</li> <li>Flexibility: Multiple pages can operate within a single browser with independent states</li> </ul> <p>Understanding these domain relationships allows you to create more efficient and well-structured automation with Pydoll.</p>"},{"location":"deep-dive/page-domain/#network-monitoring","title":"Network Monitoring","text":"<p>The Page domain implements a sophisticated network monitoring system through event collection, log storage, and response analysis.</p> <p>This enables powerful network analysis capabilities:</p> <pre><code># Enable network events\nawait page.enable_network_events()\n\n# Perform some action that triggers network requests\nawait page.go_to(\"https://example.com\")\n\n# Get network logs matching specific patterns\napi_logs = await page.get_network_logs(matches=[\"api\", \"graphql\"])\n\n# Get response bodies for specific requests\njson_responses = await page.get_network_response_bodies(matches=[\"api/data\"])\n\n# Get a specific response body by request ID\nrequest_id = api_logs[0]['params']['requestId']\nbody, is_base64 = await page.get_network_response_body(request_id)\n</code></pre> <p>Network Analysis Use Cases</p> <p>Network monitoring capabilities enable: - API response validation - Performance analysis - Data extraction from XHR responses - Debugging request/response cycles</p> <p>The implementation leverages two key components:</p> <ol> <li>Event Monitoring: Stores network events in a connection handler buffer</li> <li>Response Retrieval: Uses the Network domain to fetch response bodies when needed</li> </ol>"},{"location":"deep-dive/page-domain/#conclusion","title":"Conclusion","text":"<p>The Page domain is the central workspace for most Pydoll automation tasks. Its sophisticated architecture integrates multiple CDP domains into a unified API that simplifies complex automation scenarios while maintaining the full power of the Chrome DevTools Protocol.</p> <p>The domain's design leverages several architectural patterns: - Inheritance and composition for code organization - Command pattern for CDP communication - Observer pattern for event handling  - Factory pattern for element creation - Context managers for resource management</p> <p>By understanding the Page domain's architecture, capabilities, and patterns, you can create sophisticated browser automation scripts that effectively handle navigation, interaction, events, and state management. </p>"},{"location":"deep-dive/webelement-domain/","title":"WebElement Domain","text":"<p>The WebElement domain is a cornerstone of Pydoll's architecture, providing a rich representation of DOM elements that allows for intuitive and powerful interactions with web page components. This domain bridges the gap between high-level automation code and the underlying DOM elements rendered by the browser.</p> <pre><code>graph TB\n    Client[User Code] --&gt; Page[Page Domain]\n    Page --&gt; FindElement[FindElementsMixin]\n    FindElement --&gt; WebElement[WebElement Domain]\n    WebElement --&gt; DOM[Browser DOM]\n\n    WebElement --&gt; Properties[Properties &amp; Attributes]\n    WebElement --&gt; Interactions[User Interactions]\n    WebElement --&gt; State[Element State]\n    WebElement --&gt; TextOperations[Text Operations]\n\n    class WebElement stroke:#4CAF50,stroke-width:3px</code></pre>"},{"location":"deep-dive/webelement-domain/#understanding-webelement","title":"Understanding WebElement","text":"<p>At its core, a WebElement represents a snapshot of a DOM element within a page. Unlike traditional DOM references in JavaScript, a WebElement in Pydoll is:</p> <ol> <li>Asynchronous - All interactions follow Python's async/await pattern</li> <li>Persistent - Maintains a reference to the element across page changes</li> <li>Self-contained - Encapsulates all operations possible on a DOM element</li> <li>Intelligent - Implements specialized handling for different element types</li> </ol> <p>Each WebElement instance maintains several crucial pieces of information:</p> <pre><code>class WebElement(FindElementsMixin):\n    def __init__(\n        self,\n        object_id: str,\n        connection_handler: ConnectionHandler,\n        method: str = None,\n        selector: str = None,\n        attributes_list: list = [],\n    ):\n        self._object_id = object_id\n        self._search_method = method\n        self._selector = selector\n        self._connection_handler = connection_handler\n        self._attributes = {}\n        self._last_input = ''\n        self._command_id = 0\n        self._def_attributes(attributes_list)\n</code></pre> <p>The core components include: - The <code>object_id</code> provides a remote JavaScript reference to the element - The <code>connection_handler</code> enables communication with the browser - The <code>_search_method</code> and <code>_selector</code> track how the element was found - The <code>_attributes</code> dictionary stores element attributes - The <code>_last_input</code> remembers the last text input (useful for backspacing)</p> <p>By inheriting from <code>FindElementsMixin</code>, each WebElement can also function as a starting point for finding child elements.</p>"},{"location":"deep-dive/webelement-domain/#technical-architecture","title":"Technical Architecture","text":"<p>The WebElement domain combines several key design patterns to provide a robust and flexible API:</p> <pre><code>classDiagram\n    class WebElement {\n        -_object_id: str\n        -_search_method: str\n        -_selector: str\n        -_connection_handler: ConnectionHandler\n        -_attributes: dict\n        -_last_input: str\n        -_command_id: int\n        +click()\n        +click_using_js()\n        +type_keys(text: str)\n        +get_attribute(name: str)\n        +text\n        +inner_html\n        +value\n        +id\n        +class_name\n        +is_enabled\n    }\n\n    class FindElementsMixin {\n        +find_element(by: By, value: str, raise_exc: bool = True)\n        +find_elements(by: By, value: str, raise_exc: bool = True)\n        +wait_element(by: By, value: str, timeout: int, raise_exc: bool = True)\n        +wait_elements(by: By, value: str, timeout: int, raise_exc: bool = True)\n    }\n\n    class ConnectionHandler {\n        +execute_command(command: dict)\n    }\n\n    WebElement --|&gt; FindElementsMixin : inherits\n    WebElement *-- ConnectionHandler : uses</code></pre> <p>The architectural design follows several key principles:</p> <ol> <li>Command Pattern - Element interactions are translated into CDP commands</li> <li>Property System - Combines synchronous attribute access with asynchronous DOM property retrieval</li> <li>Mixin Inheritance - Inherits element finding capabilities through the FindElementsMixin</li> <li>Bridge Pattern - Abstracts the CDP protocol details from the user-facing API</li> </ol>"},{"location":"deep-dive/webelement-domain/#attribute-management","title":"Attribute Management","text":"<p>A unique aspect of WebElement's design is how it handles HTML attributes:</p> <pre><code>def _def_attributes(self, attributes_list: list):\n    \"\"\"\n    Defines element attributes from a flat list of key-value pairs.\n    \"\"\"\n    for i in range(0, len(attributes_list), 2):\n        key = attributes_list[i]\n        key = key if key != 'class' else 'class_name'\n        value = attributes_list[i + 1]\n        self._attributes[key] = value\n</code></pre> <p>This approach: 1. Processes attributes during element creation 2. Provides fast, synchronous access to common attributes 3. Handles Python reserved keywords (like <code>class</code> \u2192 <code>class_name</code>) 4. Forms the basis for the element's string representation</p> <p>Attribute vs. Property Access</p> <p>WebElement provides two complementary ways to access element data:</p> <ul> <li>Attribute Dictionary: Fast, synchronous access to HTML attributes available at element creation</li> <li>Asynchronous Properties: Dynamic access to current DOM state through CDP commands</li> </ul> <pre><code># Synchronous attribute access (from initial HTML)\nelement_id = element.id\nelement_class = element.class_name\n\n# Asynchronous property access (current state from DOM)\nelement_text = await element.text\nelement_bounds = await element.bounds\n</code></pre>"},{"location":"deep-dive/webelement-domain/#core-interaction-patterns","title":"Core Interaction Patterns","text":"<p>The WebElement domain provides several categories of interactions:</p>"},{"location":"deep-dive/webelement-domain/#element-properties","title":"Element Properties","text":"<p>WebElement offers both synchronous and asynchronous property access:</p> <pre><code># Synchronous properties (from attributes present at element creation)\nelement_id = element.id\nelement_class = element.class_name\nis_element_enabled = element.is_enabled\nelement_value = element.value\n\n# Asynchronous properties (retrieved from live DOM)\nelement_text = await element.text\nelement_html = await element.inner_html\nelement_bounds = await element.bounds\n</code></pre> <p>The implementation balances performance and freshness by determining which properties should be synchronous (static HTML attributes) and which should be asynchronous (dynamic DOM state):</p> <pre><code>@property\nasync def text(self) -&gt; str:\n    \"\"\"\n    Retrieves the text of the element.\n    \"\"\"\n    outer_html = await self.inner_html\n    soup = BeautifulSoup(outer_html, 'html.parser')\n    return soup.get_text(strip=True)\n\n@property\ndef id(self) -&gt; str:\n    \"\"\"\n    Retrieves the id of the element.\n    \"\"\"\n    return self._attributes.get('id')\n</code></pre>"},{"location":"deep-dive/webelement-domain/#mouse-interactions","title":"Mouse Interactions","text":"<p>WebElement provides multiple ways to interact with elements through mouse events:</p> <pre><code># Standard click at element center\nawait element.click()\n\n# Click with offset from center\nawait element.click(x_offset=10, y_offset=5)\n\n# Click with longer hold time (like for long press)\nawait element.click(hold_time=1.0)\n\n# JavaScript-based click (useful for elements that are difficult to click)\nawait element.click_using_js()\n</code></pre> <p>The implementation intelligently handles different element types and visibility states:</p> <pre><code>async def click(\n    self,\n    x_offset: int = 0,\n    y_offset: int = 0,\n    hold_time: float = 0.1,\n):\n    \"\"\"\n    Clicks on the element using mouse events.\n    \"\"\"\n    if self._is_option_tag():\n        return await self.click_option_tag()\n\n    if not await self._is_element_visible():\n        raise exceptions.ElementNotVisible(\n            'Element is not visible on the page.'\n        )\n\n    await self.scroll_into_view()\n\n    # Get element position and calculate click point\n    # ... (position calculation code)\n\n    # Send mouse press and release events\n    press_command = InputCommands.mouse_press(*position_to_click)\n    release_command = InputCommands.mouse_release(*position_to_click)\n    await self._connection_handler.execute_command(press_command)\n    await asyncio.sleep(hold_time)\n    await self._connection_handler.execute_command(release_command)\n</code></pre> <p>Special Element Handling</p> <p>The WebElement implementation includes specialized handling for different element types:</p> <pre><code># Option elements in dropdowns need special click handling\nif self._is_option_tag():\n    return await self.click_option_tag()\n\n# File inputs need special file selection handling\nawait input_element.set_input_files(\"path/to/file.pdf\")\n</code></pre>"},{"location":"deep-dive/webelement-domain/#keyboard-interactions","title":"Keyboard Interactions","text":"<p>WebElement provides multiple ways to input text into form elements:</p> <pre><code># Quick text insertion\nawait element.insert_text(\"Hello, world!\")\n\n# Realistic typing with configurable speed\nawait element.type_keys(\"Hello, world!\", interval=0.05)\n\n# Raw keyboard events\nawait element.send_keys(\"Hello\")\n\n# Special key combinations\nawait element.send_keys((\"Control\", 65))  # Ctrl+A\n\n# Clearing previous input\nawait element.backspace()\n</code></pre> <p>The implementation tracks input state to enable operations like backspacing:</p> <pre><code>async def backspace(self, interval: float = 0.1):\n    \"\"\"\n    Backspaces the key at the element.\n    \"\"\"\n    for _ in range(len(self._last_input)):\n        await self.send_keys(('Backspace', 8))\n        await asyncio.sleep(interval)\n        self._last_input = self._last_input[:-1]\n</code></pre> <p>File Upload Handling</p> <p>For file input elements, WebElement provides a specialized method:</p> <pre><code># Upload a single file\nawait file_input.set_input_files(\"path/to/file.pdf\")\n\n# Upload multiple files\nawait file_input.set_input_files([\"file1.jpg\", \"file2.jpg\"])\n\n# Using Path objects\nfrom pathlib import Path\nawait file_input.set_input_files(Path(\"path/to/file.pdf\"))\n</code></pre>"},{"location":"deep-dive/webelement-domain/#visual-capabilities","title":"Visual Capabilities","text":""},{"location":"deep-dive/webelement-domain/#element-screenshots","title":"Element Screenshots","text":"<p>WebElement can capture screenshots of specific elements:</p> <pre><code># Take a screenshot of just this element\nawait element.get_screenshot(\"element.png\")\n</code></pre> <p>This implementation involves: 1. Getting the element's bounds 2. Creating a clip region for the screenshot 3. Taking a screenshot of just that region 4. Saving the image to the specified path</p> <pre><code>async def get_screenshot(self, path: str):\n    \"\"\"\n    Takes a screenshot of the element.\n    \"\"\"\n    bounds = await self.get_bounds_using_js()\n    clip = {\n        'x': bounds['x'],\n        'y': bounds['y'],\n        'width': bounds['width'],\n        'height': bounds['height'],\n        'scale': 1,\n    }\n    screenshot = await self._connection_handler.execute_command(\n        PageCommands.screenshot(fmt='jpeg', clip=clip)\n    )\n    async with aiofiles.open(path, 'wb') as file:\n        image_bytes = decode_image_to_bytes(screenshot['result']['data'])\n        await file.write(image_bytes)\n</code></pre> <p>Multiple Bounds Methods</p> <p>WebElement provides two ways to get element bounds:</p> <pre><code># Using the DOM domain (primary method)\nbounds = await element.bounds\n\n# Fallback using JavaScript (more reliable in some cases)\nbounds = await element.get_bounds_using_js()\n</code></pre>"},{"location":"deep-dive/webelement-domain/#javascript-integration","title":"JavaScript Integration","text":"<p>WebElement provides seamless integration with JavaScript for operations that require direct DOM interaction:</p> <pre><code># Execute JavaScript in the context of this element\nawait element._execute_script(\"this.style.border = '2px solid red';\")\n\n# Get result from JavaScript execution\nvisibility = await element._is_element_visible()\n</code></pre> <p>The implementation uses the CDP Runtime domain to execute JavaScript with the element as the context:</p> <pre><code>async def _execute_script(\n    self, script: str, return_by_value: bool = False\n):\n    \"\"\"\n    Executes a JavaScript script in the context of this element.\n    \"\"\"\n    return await self._execute_command(\n        RuntimeCommands.call_function_on(\n            self._object_id, script, return_by_value\n        )\n    )\n</code></pre>"},{"location":"deep-dive/webelement-domain/#element-state-verification","title":"Element State Verification","text":"<p>WebElement provides methods to check the element's visibility and interactability:</p> <pre><code># Check if element is visible\nis_visible = await element._is_element_visible()\n\n# Check if element is the topmost at its position\nis_on_top = await element._is_element_on_top()\n</code></pre> <p>These verifications are crucial for reliable automation, ensuring that elements can be interacted with before attempting operations.</p>"},{"location":"deep-dive/webelement-domain/#position-and-scrolling","title":"Position and Scrolling","text":"<p>The WebElement domain includes methods for positioning and scrolling:</p> <pre><code># Scroll element into view\nawait element.scroll_into_view()\n\n# Get element bounds\nbounds = await element.bounds\n</code></pre> <p>These capabilities ensure that elements are visible in the viewport before interaction, mimicking how a real user would interact with a page.</p>"},{"location":"deep-dive/webelement-domain/#performance-and-reliability-considerations","title":"Performance and Reliability Considerations","text":"<p>The WebElement domain balances performance and reliability through several key strategies:</p>"},{"location":"deep-dive/webelement-domain/#smart-fallbacks","title":"Smart Fallbacks","text":"<p>Many methods implement multiple approaches to ensure operations succeed even in challenging scenarios:</p> <pre><code>async def click(self, ...):\n    # Try using CDP mouse events first\n    # If that fails, fallback to JavaScript click\n    # If that fails, provide a clear error message\n</code></pre>"},{"location":"deep-dive/webelement-domain/#appropriate-context-selection","title":"Appropriate Context Selection","text":"<p>The implementation chooses the most appropriate context for each operation:</p> Operation Approach Rationale Get Text Parse HTML with BeautifulSoup More accurate text extraction Click Mouse events via CDP Most realistic user simulation Select Option Specialized JavaScript Required for dropdown elements Check Visibility JavaScript Most reliable across browser variations"},{"location":"deep-dive/webelement-domain/#command-batching","title":"Command Batching","text":"<p>Where possible, operations are combined to reduce round-trips to the browser:</p> <pre><code># Get element bounds in a single operation\nbounds = await element.get_bounds_using_js()\n\n# Calculate position in local code without additional browser calls\nposition_to_click = (\n    bounds['x'] + bounds['width'] / 2,\n    bounds['y'] + bounds['height'] / 2,\n)\n</code></pre>"},{"location":"deep-dive/webelement-domain/#conclusion","title":"Conclusion","text":"<p>The WebElement domain provides a comprehensive and intuitive interface for interacting with elements in a web page. By encapsulating the complexities of DOM interaction, event handling, and state management, it allows automation code to focus on high-level tasks rather than low-level details.</p> <p>The domain demonstrates several key design principles:</p> <ol> <li>Abstraction - Hides the complexity of CDP commands behind a clean API</li> <li>Specialization - Provides unique handling for different element types</li> <li>Hybrid Access - Balances synchronous and asynchronous operations for optimal performance</li> <li>Resilience - Implements fallback strategies for common operations</li> </ol> <p>When used in conjunction with the Page domain and Browser domain, WebElement creates a powerful toolset for web automation that handles the complexities of modern web applications while providing a straightforward and reliable API for developers. </p>"}]}